#!/usr/bin/env ruby
# snow - Command suite to support moving to snowflake
#
# This script grew into the monstrosity you see before you now: the center of a
# command/workflow suite. The quick and dirty version:
#
# 1. Run this from inside your docker container.
# 2. $ set-ds-script path/to/job_file.py
# 3. $ snow
#
# Running `snow` will run all of the redshift->snowflake problem detectors and print their output.
#
# `snow run` will run the current script, and will try to autodetect necessary
# parameters: if the script accepts a --test or --recipients option, this script
# will add those parameters.

# a job name.
#
# DONE: snow -q, --quick emits only the detectors that fail, so you can see
# remaining work
#
# DONE: port this to Optimist*. The arg processing is killing me here
#
# TODO: instead of catching individual commands, take ARGV.first and check for
# an executable "snow-<arg>" script in bin
#
# TODO: Document the snow ecosystem for future me:
#
#     This space unintentionally, but entirely predictably, left blank
#
# TODO: Keep the job name around when off of the branch. Now we have the ability
# to blast a flash warning on the screen saying "you're off-branch, are you
# sure?". That would be more convenient that running python manually--and losing
# the checks in the process.
#
# TODO: Write fixes to a script file instead of just the console. Hand-copying
# 40 sed commonds to fix timezones has the sheets all the way up my crack.
#
# - snow - check the current script. -q to only show failing checks.
# - snow main - jump back to main snowflake branch
# - snow master - same thing but back to origin/master
# - snow new <id> <file> - create new branches
# - snow run
# - snow-commit-warehouse-* will commit the script with a specific message. It
#     does not modify the script.
# - scow <term> will show you all snow-commit-warehouse-*<term>* files, let you
#     pick one with selecta, then run it.
# - snow-fix-* will modify the script AND commit it.

require 'colorize'
require 'fileutils'
require 'io/console'
require 'json'
require 'optimist'

String.disable_colorization unless $stdout.tty?

class SnowflakeManglerApp
  attr_reader :opts
  attr_accessor :argv

  def checklist
    run_and_exit! "snow-show-checklist #{argv.join(' ')}"
  end

  def set_script(script)
    @get_script = script
  end

  def get_script
    return @get_script if @get_script

    @get_script = `get-ds-script`.strip
    @get_script = ENV['SCRIPT'] if @get_script.empty?
    @get_script
  end

  # Some commands do not need the script var to be set; in fact they need to be
  # able to when it is not set (because they cause it to become set)
  def need_script?
    no_need = (["backlog", "go", "link", "new", "master", "main", "cplx", "complexity"] & argv).size > 0 ||
              (argv.first == "job" && argv.size >= 2)
    !no_need
  end

  def excel_automation?
    get_script == "etl_automated_excel_reports.py"
  end

  def sql_file?
    argv.last && argv.last.end_with?(".sql")
  end

  def osx?
    `uname -s`.strip == "Darwin"
  end

  def docker?
    ENV['IS_DOCKER_LOCAL'] = "1"
  end

  # ----------------------------------------------------------------------
  # OPTIONS ACCESSORS - Because this script made it 6 months and 1100 LOC before
  # I added the Optimist gem and by then it was detecting arguments by Doing Its
  # Own Thing Man(TM)
  # ----------------------------------------------------------------------

  def explicitly_allow_redshift?
    opts[:redshift_given] && opts[:redshift]
  end

  # force: proceed in the face of warnings
  def force?
    opts[:force]
  end

  # Quick: Don't show passing detectors or extraneous logging
  def quick?
    opts[:quick]
  end

  # Quiet: Suppress output, return bash success status only
  def quiet?
    opts[:quiet]
  end

  # For job config only: show the default job config AND this job config (to show complete config)
  def defaults?
    opts[:defaults]
  end

  # Was this script invoked with --essential?
  def essential?
    opts[:essential]
  end

  # Was this script invoked with --test?
  def test?
    opts[:test]
  end

  # Was this script invoked with --send-test (or --no-send-test)?
  def send_test?
    opts[:"send-test"]
  end

  # Was this script invoked with --list?
  def list?
    opts[:list]
  end

  def preview?
    argv.include?("preview")
  end

  def job?
    argv.include?("job")
  end

  def run_timezone_detectors?
    opts[:timezone]
  end

  def run_test_detectors?
    opts[:test]
  end

  def run_to_char_detectors?
    opts[:char]
  end

  def run_python_detectors?
    opts[:python]
  end

  def run_python2_detectors?
    opts[:python2]
  end

  def run_json_detectors?
    opts[:json]
  end

  def run_missing_detectors?
    opts[:missing]
  end

  def run_whitespace_detectors?
    opts[:space]
  end

  def run_quotes_detectors?
    opts[:quotes]
  end

  def run_date_detectors?
    opts[:dates]
    # 2023-03-08: Should I maybe do opts[:disable].split(/,/).include?("date")
    # So I can say --disable=date,json etc?
    #
    # Right now I kinda like --no-json --no-dates, so eh. But if it tuns into 20
    # different --no-foo options, this might be the way.
    #
    # 2023-04-13: Dear March Dave, This is April Dave. Hello from the future!
    # Yeah, it's turning into a shipwreck of options.
  end

  def run_days_detectors?
    opts[:days]
  end

  def run_exception_detectors?
    opts[:exceptions]
  end

  # show_fixes_only: suppress output except for fixes, i.e. spit out a big list
  # of sed commands
  def show_fixes_only?
    opts[:fixes]
  end

  def use_test_mode?
    opts[:test]
  end

  def run_trunc_detectors?
    opts[:trunc]
  end

  # ----------------------------------------------------------------------
  # END OPTIONS ACCESSORS
  # ----------------------------------------------------------------------

  def section_title(title)
    "====%s" % " #{title} ".ljust(80, "=").cyan
  end

  def seen_errors!
    @seen_errors = true
  end

  def seen_errors?
    !!@seen_errors
  end

  def log_error(msg)
    seen_errors!
    log(msg) unless show_fixes_only?
  end

  def log(msg)
    puts msg unless quiet? || show_fixes_only?
  end

  # Like log, but doesn't balk at show_fixes_only
  def log_fix(msg)
    puts msg
  end

  def log_ok
    return if quick?
    log " OK ".bold.light_white.on_green
  end

  # ======================================================================
  # DETECTORS
  # ======================================================================

  # This is turning into a bit of a swiss army kitchen sink...
  # lines:     array of lines to check
  # title:     section title to print if matches are found
  # regex:     use this to detect matching lines. If regex is an array of regexes,
  #            finds matches on adjacent lines
  # disabled:  this check is turned off. Do not run, even if marked essential
  # essential: this check is essential. If we're running snow -e, skip
  #            inessential checks
  # autofix:   text containing an autofix. Usually a bash command applied to the
  #            whole file
  # suggest:   a method that receives the matching line, and returns a suggested
  #            fix.
  # unmatch:   ignore lines that match regex but ALSO match this regex.
  # warning:   Warning only and/or the detector will continue to fire even after
  #            the problem has been fixed
  def detect_and_print_lines(
        lines,
        title,
        regex,
        essential: false,
        disabled: false,
        autofix: nil,
        suggest: nil,
        unmatch: nil,
        warning: nil
      )
    return if disabled
    return if essential? && !essential
    log section_title title unless quick?
    # OOF.
    # match the regex AND (we have no unmatch, or line is not unmatched)
    if lines.any? { |line| line =~ regex && (!unmatch || line !~ unmatch) }
      log section_title title if quick?
      if autofix
        log "Autofix with:"
        log_fix autofix.cyan
        fixlog "#{get_script}: autofix #{title}", autofix
      end

      # Yeah, turn this into lines_with_indexes = finder_method, we can
      # write a generic version to do what this regex does, and then we can
      # pass in custom finder methods for multiline and trickier stuff
      lines.each.with_index do |line, index|
        if line =~ regex && (!unmatch || line !~ unmatch)
          seen_errors!
          log "%4d: %s" % [index+1, line]
          if suggest && suggestion = send(suggest, line)
            log "Replacement:"
            log line.red
            log suggestion.green
            sed_replacement = suggestion.gsub(%r|/|, "\\/").gsub('"', '\"')

            sed_command = "sed -i -E \"#{index+1}s/^.*$/#{sed_replacement}/\" #{get_script}"
            log_fix sed_command.cyan
            fixlog "#{get_script}: replacement for #{title}", sed_command
          end
        end
      end
    else
      log_ok
    end
  end

  def detect_and_print_multilines(
        lines,
        title,
        regexes,
        essential: false,
        disabled: false,
        autofix: nil,
        suggest: nil,
        unmatch: nil
      )
    return if disabled
    return if essential? && !essential
    log section_title title unless quick?

    # given n regexes, at line[m], then...
    # Simple Match For Now:
    # 1. lines[m..m+n].zip(regexes).all? {|line, regex| line =~ regex }
    #
    # More complex matching might allows for same-line stuff, e.g. if we are
    # matching on [/create/, /table/, /like/], we should match if:
    #
    # (line[m] =~ /create.*table.*like/) ||
    # (line[n] =~ /create/ && line[n+1] =~ /table.*like/) ||
    # (line[n] =~ /create.*table/ && line[n+1] =~ /like/) ||
    # (line[n] =~ /create/ && line[n+1] =~ /table/ && line[n+2] =~ /like/)
    no_matches = true
    first = true
    0.upto(lines.size - regexes.size) do |main_offset|
      if (0...regexes.size).map {|j| [lines[main_offset+j], regexes[j]] }.all? {|line, regex| line =~ regex }
        no_matches = false
        if first
          first = false
        else
          log "--"
        end
        log (0...regexes.size).map {|j| "#{main_offset+j}: #{lines[main_offset+j]}" }
      end
    end

    no_matches ? log_ok : seen_errors!
  end

  # Let's start capturing the fixes and suggests to a script/log
  def fixlog(comment, autofix)
    # 2023-06-28 This is pretty sketchy and keeps going sideways on me.  Keeping
    # it around for now because it's for the sql submodule and we're working on
    # that now, but this code may be past its sell-by date.
    # comment = comment.gsub(%r|sql/|, '')
    # autofix = autofix.gsub(%r|sql/|, '')

    # autofix_filename = File.expand_path("./autofixes/#{get_script}.sh")
    # autofix_folder = File.dirname(autofix_filename)
    # FileUtils.mkdir_p(autofix_folder) unless Dir.exists?(autofix_folder)
    # if !File.exist?(autofix_filename)
    #   File.open(autofix_filename, "w") { |file| file.puts "#!/bin/bash"}
    # end
    # File.open(autofix_filename, "a") { |file| file.puts "# #{comment}"; file.puts autofix}
  end

  def detect_missing_skip_upload
    log section_title "Skip Upload" unless quick?
    if should_have_skip_upload? && !has_skip_upload?
      seen_errors!
      log section_title "Skip Upload" if quick?
      log "File is mccloud or salesforce but does not have --skip_upload option"
    else
      log_ok
    end
  end

  def detect_missing_test_mode
    log section_title "--test" unless quick?
    if should_have_test_mode? && !has_test_mode?
      seen_errors!
      log section_title "--test" if quick?
      log "File mentions portal, gsheet, facebook, or commissions but does not have --test option"
    else
      log_ok
    end
  end

  def detect_missing_recipients_override
    if should_have_recipients_override? && !has_recipients_override?
      seen_errors!
      log section_title "--recipients" if quick?
      log "File sends email but does not have --recipients override option"
    else
      log_ok
    end
  end

  def detect_unmanaged_whitespace(lines)
    log section_title "UNMANAGED WHITESPACE" unless quick?
    if lines.any? {|line| line =~ /\s+$/}
      seen_errors!
      log section_title "Unmanaged whitespace" if quick?
      lines.each.with_index do |line, index|
        if line =~ /(\s+)$/
          colorized_line = line.sub(/\s+$/, $1.on_red)
          log "%d: %s" % [index+1, colorized_line]
        end
      end
    else
      log_ok
    end
  end

  def run_pep_check?
    opts[:pep]
  end

  # ======================================================================
  # END DETECTORS
  # ======================================================================

  def pointed_at_redshift?
    `snow-job-show-config`.strip.each_line.any? {|line| line =~ /"dst_conn": .*redshift/ }
  end

  def on_master_branch?
    `git current-branch`.strip == "master"
  end

  def has_skip_upload?
    IO.readlines(get_script).any? { |line| line.include?("parser.add_argument('--skip_upload") }
  end

  def should_have_skip_upload?
    get_script.include?("_sf_") ||
      get_script.include?("salesforce") ||
      (get_script.include?("mcloud") && !get_script.include?("_get_"))
  end

  def has_test_mode?
    File.readlines(get_script).any? {|line| line.include?("parser.add_argument('--test")}
  end

  def has_start_date?
    File.readlines(get_script).any? {|line| line.include?("parser.add_argument('--start_date")}
  end

  def has_end_date?
    File.readlines(get_script).any? {|line| line.include?("parser.add_argument('--end_date")}
  end

  def should_have_test_mode?
    # this is just a tiny tiny sample
    ["gsheet", "facebook", "commissions"].any? {|part| get_script.include?(part) } ||
      File.readlines(get_script).any? {|line| line.include? "portal" }
  end

  def has_recipients_override?
    File.readlines(get_script).any? {|line| line.include? "parser.add_argument('--recipients'" }
  end

  def should_have_recipients_override?
    # true if we see "mail_recipients" anywhere in the file
    # -OR- if we see "mail_recipients" in the job config
    File.readlines(get_script).any? {|line| line.include? "mail_recipients" }
  end

  def run_and_exit!(command)
    system command
    exit $?.exitstatus
  end

  def usage
    str = <<~USAGE
snow - master control script for snowflake job migration

snow [<command> [<options>]]

Most commands use get-ds-script, which checks the last line of ~/.ds_script for
the name of the script filename, you'll want to set this with `set-ds-script
path/to/job.py` beforehand.

Many jobs check that the current branch matches the output of
get-ds-script. This is mostly to prevent jamming changes directly into master or
feature/snowflake-moveover. This log an extra bit of bookkeeping on you, but
it's actually there to ensure that I'm not accidentally merging to master or
snowflake-moveover. Again.

Many commands now accept a job override, notably job (shows the job config) and preview (

Run snow -l to get a checklist of the workflow.

Commands/options:
auto                                 Automatically fix and commit as much as possible
backlog                              Show backlog file
check [-e|--essential] [job.py]      Check the [current] job for [only essential] coding violations
complexity|cplx [job.py]             Give a rough complexity estimate for porting this job
go <ticket>|<job_name>               Checkout (in both repos) the branch containing this search term
job [job]                            Show current [or specified] job's configuration
job r[edshift]                       Convert job configuration back to redshift
job s[nowflake]                      Convert job configuration to snowflake
link <pr> <finance_pr> [package_pr]  Create Slackable links for Zack
link <pr> <finance_pr> [pkg_pr]      Create links template for pasting to Zack in slack
main                                 Checkout the snowflake main branch in both repos
master                               Checkout master in both repos
new <ticket> <job.py>                Create a new feature branch in both repos
preview job.py                       Run check on a job other that the current script
rebase [parent_branch]               Run snow-rebase
run                                  Run the current job
slam                                 Commit finance config change, push both repos, and generate click-to-open-PR links

If no commands are given, snow will run check.

Options:
    USAGE
    # actual options are automagically presented by Optimist after displaying this banner text
  end

  def replace_at_time_zone(line)
    # AND t.contact_start AT TIME ZONE 'UTC' AT TIME ZONE 'America/Denver' >= '2021-01-23'
    # ,MIN(v.created_at) AT TIME ZONE 'UTC' AT TIME ZONE 'America/Denver' AS awaiting_delivery_at_local
    #
    # in a subselect with double quotes and doubled single quotes
    # ,cc.updated_at AT TIME ZONE ''UTC'' AT TIME ZONE ''America/Denver'' AS "Updated Date"
    getdate_regex = %r|^(.*)GETDATE\(\) AT TIME ZONE '+UTC'+ AT TIME ZONE '+America/Denver'+(.*)$|i
    if line =~ getdate_regex
      # if we find a match to this, return it now
      return line.sub(getdate_regex, %q|\1CONVERT_TIMEZONE('UTC', 'America/Denver', GETDATE())\2|)
    end

    regex = /^(\s*,?)([^,]+) AT TIME ZONE ('+[^']+'+) AT TIME ZONE ('+[^']+'+)/i
    #           ^      ^                     ^                          ^
    #           |      |                     |                           \_ $4. "to" time zone
    #           |      |                      \_ $3. "from" time zone
    #           |       \_ $2. timestamp column to be converted
    #            \_ $1. indentation and optional leading comma
    #

    return unless line =~ regex

    # line.gsub(regex, "#{$1}CONVERT_TIMEZONE(#{$3}, #{$4}, #{$2})")
    line = line.gsub(regex, '\1CONVERT_TIMEZONE(\3, \4, \2)')

    # Sometimes we get extra parens around the timestamp, like so:
    # CONVERT_BLAH(x, y, (c.contact_start))

    # This is tricky to catch going in because the regex breaks $2 right before
    # the closing paren. However, these are detectable after the replacement
    # because the innermost match has no whitespace or parens.
    line.gsub( /\(([A-Za-z0-9_\.]+)\)/, '\1' )
  end

  # This doesn't get all the cases, but it does get the ones that are inline.
  def replace_inline_create_table_like(line)
    regex = /\(LIKE (.+?)\)/
    return unless line =~ regex

    line.gsub(regex, "LIKE #{$1}")
  end

  def replace_epoch_timestamp_with_interval_1_second(line)
    regex = /timestamp\s+'epoch'\s*\+\s*(.*?)\*\s*interval\s+'1 second'/i

    return unless line =~ regex

    replacement = $1.strip
    replacement = replacement[1..] if replacement.start_with? "("
    replacement = replacement[..-2] if replacement.end_with? ")"
    replacement = replacement.strip

    replacement = "TO_TIMESTAMP(#{replacement})"

    line.gsub(regex, replacement)
  end

  def replace_similar_to_with_like_any(line)
    regex = /^(.*?)([A-Za-z0-9\._]+)\s+((NOT)\s+)SIMILAR\s+TO\s+(.+)/i
    #    if line =~ regex
    return unless line =~ regex

    # column, is_not, patterns = $1, $3, $4
    left, column, is_not, patterns = $1, $2, $4, $5

    return unless line =~ regex

    replacement = "column: #{column}, not: #{is_not ? 'NOT!' : 'not-not'}, patterns: #{patterns.inspect}"

    pats = if patterns =~ /^'%\((.*?)\)%'$/
             $1.split(/\|/).map {|pat| "'%#{pat}%'"}.join(', ')
           else
             "NO MATCH"
           end


    "#{left}#{is_not ? 'NOT ' : ''}#{column} LIKE ANY (#{pats})"

    # x SIMILAR TO '%(pants)%' => x LIKE '%pants%'
    # x SIMILAR TO '%%(pants)%%' => x LIKE '%pants%'
    # x SIMILAR TO '%(hat|glasses)%' => x LIKE ANY ('%hat%', '%glasses%')

    # To negate, put the NOT in front of the column name
    # x NOT SIMILAR TO '%(pants)%' => NOT x LIKE ANY ('%pants%')

    # match_phrase = \
    # if match begins and ends with %(...)% and contains no other %'s
    #   strip leading %( and trailing )%
    #   # is match plural?
    #   matches = match.split(%r|/|).map {|m| "'%#{m}%'" }.join(", ")
    #   "LIKE ANY (#{matches})"
    #
    # else
    #   it's a weird match, let the human do it

    # line
    #   .sub(regex, "RLIKE(#{$2})")
    #   .sub(%r{'%\(?}, "'.*(")
    #   .sub(%r{\)?%'}, ").*'")
    #   .gsub(/\+/, '\\\\+')
  end


  # THE MAIN APP METHOD, LET'S GO
  def run
    banner_text=usage # bc we can't call self.usage inside this do...end (it changes self)
    @opts = Optimist.options do
      banner banner_text

      @ignore_invalid_options = true

      opt :"send-test", "Send --test in snow run to jobs that offer it", short: :none, default: true
      opt :debug, "Output extra debug info", short: :none
      opt :defaults, "(job only) Include job config defaults with job config (to see complete config)", short: :d
      opt :essential, "Only run essential detectors"
      opt :force, "Force continuation over warnings, such as get-ds-script not pointing at an actual file. Also send -f to git checkout (for/if submodule errors)"
      opt :list, "Show PR workflow checklist"
      opt :redshift, "Explicitly allow job to target redshift", short: :none, default: false

      opt :quick, "(run only) Only show output from failing detectors", short: "Q"
      opt :quiet, "Suppress all output if possible, and return success exit status if all detectors pass (or print 'OK' if output is not piped)", short: :Q, default: false
      opt :report, "(run only) Report file for etl_automated_excel_reports)", short: :none, type: :string
      opt :fixes, "(check|fix only) Only output fixes", short: :none, default: false

      # Commands only for passing through
      opt :clear, "Clear ~/.ds_script and ~/.ds_table cache files (snow go only)", short: :none, default: false

      # TODO: change these to arguments to a --skip command? That would make it
      # easier to write an --only command...
      opt :char, "Run TO_CHAR detectors", default: true
      opt :days, "Run DAYS AS detectors", default: true
      opt :exception, "Run exception detectors", default: true
      opt :json, "Run JSON detectors", short: :none, default: true
      opt :missing, "Run missing detectors", short: :none, default: true
      opt :python, "Run python code detectors", default: true
      opt :python2, "Run python2 string formatting detectors", default: true
      opt :quote, "Run double-quote detectors", default: true
      opt :test, "Run test mode detector", default: true
      opt :timezone, "Run time zone detectors", default: true
      opt :trunc, "Run trunc detectors", default: true

      opt :"no-2", "same as --no-python2", short: :none, default: false
      opt :"no-p", "same as --no-python", short: :none, default: false
      opt :"no-c", "same as --no-char", short: :none, default: false
      opt :"no-d", "same as --no-days", short: :none, default: false
      opt :"no-j", "same as --no-json", short: :none, default: false
      opt :"no-m", "same as --no-missing", short: :none, default: false
      opt :"no-q", "same as --no-quotes", short: :none, default: false
      opt :"no-s", "same as --no-space", short: :none, default: false
      opt :"no-t", "same as --no-test", short: :none, default: false
      opt :"no-tr", "same ass --no-trunc", short: :none, default: false
      opt :"no-tz", "same as --no-timezone", short: :none, default: false
      opt :"no-x", "same as --no-exception", short: :none, default: false
      opt :space, "Run unmanaged whitespace detectors", default: true

      # TODO: redo this as sthg like pass_through_args(:start, :end, etc...)
      opt :start, "Start (pass through to etl_nightly.py)", type: :string
      opt :end, "End (pass through to etl_nightly.py)", type: :string
    end
    # copy alternate args over, so that e.g. --no-j will set --no-json to true
    {
      char: "c",
      days: "d",
      exception: "x",
      json: "j",
      missing: "m",
      python: "p",
      python2: "2",
      quote: "q",
      space: "s",
      test: "t",
      timezone: "tz",
      trunc: "tr",
    }.each do |option, shortcode|
      # opts[:json] = opts[:"no-j"] if opts[:"no-j_given"]
      opts[option] = opts[:"no-#{shortcode}"] if opts[:"no-#{shortcode}_given"]
    end

    log JSON.pretty_generate(opts) if opts[:debug]
    @argv = ARGV.dup

    # Optimist::die :dynos, "must be a non-negative number" unless opts[:dynos] >= 0.0

    # if -l or --list, show checklist and exit
    if list?
      checklist
      # log "HIT ENTER TO EXIT"
      # $stdin.gets
      exit 0
    end

    # Override script if job is preview
    if preview?
      # If arg 1 is an integer, we've been given a ticket number also
      set_script((argv[1].to_i > 0) ? argv[2] : argv[1])
    end

    # Optionally override script if job is show config for job
    if job?
      # If arg 2 exists and is a file that exists, override it
      if argv[2] && File.exist?(argv[2])
        set_script(argv[2])
      end
    end

    # 2.2 If you do not have SCRIPT set, I tell you about SCRIPT and exit
    if need_script? && (get_script.nil? || get_script.empty?)
      log "You must run set-ds-script <path/to/job.py> first."
      usage
      exit -1
    end

    # If the script is a sql file, disable code and whitespace detectors
    if sql_file?
      opts[:space] = false
      opts[:python] = false
    end

    # ----------------------------------------------------------------------
    # 3. dispatch/handle commands
    # ----------------------------------------------------------------------

    # ----------------------------------------------------------------------
    # snow run -> dsetl_nightly
    # ----------------------------------------------------------------------
    if argv.first == "run"
      if !docker?
        log "ETL jobs must be run from inside docker.".bold.red
        exit -1
      end

      # If the job config is pointed at redshift, the branch must be master OR I
      # must explicitly specify --redshift
      if pointed_at_redshift? && (!on_master_branch? && !explicitly_allow_redshift?)
        log "JOB IS POINTED AT REDSHIFT, AND YOU ARE ON A STORY BRANCH. Should you be on Snowflake? If you're sure you meant redshift, you can override this check with --redshift".bold.red
        exit -1
      end

      # You must be in the warehouse folder. Too many of the quick-and-dirty
      # early versions of these scripts assume this and reference files
      # relatively.
      if File.expand_path(Dir.pwd) != File.expand_path("~/data_services/etl/warehouse/")
        log "You must be in the warehouse folder for snow run to work reliably.".bold.red
        exit -1
      end

      # Check to see if etl_nightly.py has been gorbled by git, which has
      # happened twice in the last week.
      #
      # "If I had a nickel for every time [git has trashed etl_nightly.py], I'd
      # have two nickels - which isn't a lot, but it's weird that it happened
      # twice."
      if !system("grep -- '--retry_count' ~/data_services/etl/warehouse/etl_nightly.py > /dev/null")
        log "etl_nightly.py has no mention of the retry_count parameter. Is the finance submodule out of sync?".bold.white.on_red
        log
      end

      # print dst_conn warehouse name in top-right corner
      system("snow-get-warehouse -c") unless quick?

      # TODO: I would like to add -c to 'snow run' to "show command only",
      # but the showing of the command is entirely inside dsetl_nightly. So we
      # generate all the arguments here, but we display them over there. This is
      # the opposite of SRP--a single concern has been separated into two
      # separate programs.
      argv.shift # clip "run" off of the argv
      command = "dsetl_nightly"
      command += " --skip_upload" if has_skip_upload?
      command += " --force" if force?
      command += " --test" if has_test_mode? && use_test_mode? && send_test?
      command += " --start_date=#{(Date.today - 60).strftime('%F')}" if has_start_date? && !opts[:start_date_given] # argv.include?('--start_date')
      command += " --end_date=#{(Date.today - 59).strftime('%F')}" if has_end_date? && !opts[:end_date_given] # !argv.include?('--end_date')

      # TODO: redo this as sthg like pass_through_args(:start, :end, etc...)
      command += " --start=#{opts[:start]}" if opts[:start_given]
      command += " --end=#{opts[:end]}" if opts[:end_given]
      command += " --recipients=#{`git config user.email`.strip}" if has_recipients_override? && !argv.include?('--recipients')

      # snatch report sql into --report if we're running excel automation
      opts[:report] = argv.pop if excel_automation? && sql_file?

      if opts[:report]
        report = opts[:report].sub('sql/', '').sub('.sql', '')
        command += " --report=#{report}"
      end

      command += " " + argv.join(" ")
      run_and_exit! command
    end

    # ----------------------------------------------------------------------
    # snow main -> snow-go-main
    # ----------------------------------------------------------------------
    if argv.first == "main"
      command = "snow-go-main"
      command += " --clear" if opts[:clear]
      puts command.cyan
      run_and_exit! command
    end

    # ----------------------------------------------------------------------
    # snow master -> snow-go-master
    # ----------------------------------------------------------------------
    if argv.first == "master"
      command = "snow-go-master"
      command += " --clear" if opts[:clear]
      puts command.cyan
      run_and_exit! command
    end

    # ----------------------------------------------------------------------
    # snow go <id> -> snow-go-branch
    # ----------------------------------------------------------------------
    if argv.first == "go"
      command = "snow-go-branch"
      command += " --force" if opts[:force]
      command += " #{argv[1..] * ' '}"
      puts command.cyan
      run_and_exit! command.strip
    end

    # ----------------------------------------------------------------------
    # snow rebase [parent_branch] -> snow-rebase
    # ----------------------------------------------------------------------
    if argv.first == "rebase"
      run_and_exit! "snow-rebase #{argv[1..] * ' '}"
    end

    # ----------------------------------------------------------------------
    # snow cplx | complexity
    # ----------------------------------------------------------------------
    if (argv & ["cplx", "complexity"]).size > 0
      run_and_exit! "snow-complexity #{argv[1..] * ' '}"
    end

    # ----------------------------------------------------------------------
    # snow job -> snow-show-job-config
    # ----------------------------------------------------------------------
    if argv.first == "job"
      if defaults?
        @argv = argv.dup - ["defaults", "d", "--defaults"]
        run_and_exit! "snow-job-show-config --defaults #{argv[1..].join(' ')}"
      elsif argv[1] == "redshift" || argv[1] == "r"
        run_and_exit! "snow-job-convert-to-redshift #{get_script}"
      elsif argv[1] == "snowflake" || argv[1] == "s"
        run_and_exit! "snow-job-convert-to-snowflake #{get_script}"
      else
        run_and_exit! "snow-job-show-config #{argv[1..].join(' ')}"
      end
    end

    # ----------------------------------------------------------------------
    # snow new <ticket> <script> -> run snow-new-branch, which runs git new-branch in both repos
    # ----------------------------------------------------------------------
    if argv.first == "new"
      run_and_exit! "snow-new-branch #{argv[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow link <pr> <finance_pr> [package_pr] -> Create Slackable links for Zack
    # ----------------------------------------------------------------------
    if argv.first == "link"
      run_and_exit! "snow-link #{argv[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow backlog - show backlog file
    # ----------------------------------------------------------------------
    if argv.first == "backlog"
      run_and_exit! "snow-backlog #{argv[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow [check] [-e]
    # ----------------------------------------------------------------------

    if argv.first == "check"
      args = argv.dup
      args -= ['-e','--essential']

      if args.size > 1
        set_script args.last
      end
    end

    # ----------------------------------------------------------------------
    # snow slam - git cram both repos and emit create pr links
    # ----------------------------------------------------------------------
    if argv.first == "slam"
      run_and_exit! "snow-slam #{argv[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow auto - automatically fix what can be automatically fixed
    # ----------------------------------------------------------------------
    if argv.first == "auto"
      run_and_exit! "snow-autofix-all-the-things #{argv[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow check - run detectors against job
    # ----------------------------------------------------------------------
    log "Checking #{get_script}..."

    # load file
    lines = IO.readlines(get_script).map(&:chomp)

    # ----------------------------------------------------------------------
    # Check Whitespace
    # ----------------------------------------------------------------------
    detect_unmanaged_whitespace lines if run_whitespace_detectors?

    # ----------------------------------------------------------------------
    # Check for PEP8 Conformance
    # ----------------------------------------------------------------------
    if !essential? && !quick?
      if run_pep_check?
        log section_title("PEP8 CONFORMANCE")
        pep_ok = system "pep-check #{get_script}"
        seen_errors! unless pep_ok
      else
        log section_title("SKIPPING PEP8 CONFORMANCE")
      end
    end

    # ======================================================================
    # BEGIN SQL CHANGES - Stuff that runs in Redshift but not in Snowflake
    # ======================================================================

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: DISTKEY, SORTKEY
    # ----------------------------------------------------------------------
    # DISTKEY, SORTKEY: just remove, it's fine
    detect_and_print_lines lines, "DISTKEY / SORTKEYS", /\b(DIS|SOR)TKEY\b/i, essential: true, autofix: "sed -E -i '/^.\\s*\\b(DIS|SOR)TKEY\\b .*$/d' #{get_script}"
    # if no conflicts found, this can be autofixed with sed -E -i '/\b(DIS|SOR)TKEY\b/d

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: * INTERVAL
    # ----------------------------------------------------------------------
    #
    # Don't use postgresq2l date interval arithmetic.
    #
    # Instead of
    # WHERE timestamp::DATE >= CURRENT_DATE - INTERVAL '2 days'
    #
    # We have to use DATEADD or DATEDIFF:
    #
    # WHERE timestamp >= DATEADD(day, -2, CURRENT_DATE)
    # WHERE DATEDIFF(day, timestamp, CURRENT_DATE) <= 2
    #
    # Or
    #
    # ==> LQQK LQQK LQQK ==> Watch out: This is an ok conversion:
    # contact_start - INTERVAL '6 hour'
    #
    # Watch for Spencer's `timestamp 'epoch' + seconds * interval '1 second'`,
    # it specifally converts easilly to just `TO_TIMESTAMP(seconds)`
    detect_and_print_lines lines, "Interval Arithmetic",
                           /\*\s*INTERVAL\b/i,
                           essential: true,
                           suggest: :replace_epoch_timestamp_with_interval_1_second

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: Postgres INT2, INT4, INT8
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Postgresql integer extension types", /\bINT(2|4|8)\b/i, essential: true

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: GEOGRAPHY
    # ----------------------------------------------------------------------
    # GEOGRAPHY data type: change to GEOMETRY data type
    detect_and_print_lines lines, "GEOGRAPHY", /\bGEOGRAPHY\b/i, essential: true

    if run_timezone_detectors?
      # ----------------------------------------------------------------------
      # Illegal Snowflake Syntax: AT TIME ZONE
      # ----------------------------------------------------------------------
      # AT TIME ZONE '<zone>': change to CONVERT_TIMEZONE('<to_zone>', source_timestamp)
      #
      # Can this be automated? NO, this is some serious bullshit.
      #
      # DANGER DANGER DANGER:  Investigate. My snowflake sessions in DataGrip are
      # set to MST timezone. I fixed this (for how long? Guessing for the length
      # of "the session") with ALTER SESSION SET TIMEZONE = 'UTC'.
      #
      # let's say time = TO_TIMESTAMP('2022-09-13 09:00:00 +00:00'). That works in
      # both databases.
      #
      # Redshift's TO_TIMESTAMP will return a timestamp WITH TIMEZONE, so later on
      # doing time AT TIME ZONE 'MST' convert TO MST, subtract 7 hours, and drop
      # the TZ part. BUT!  Snowflake's TO_TIMESTAMP will return a timestamp
      # WITHOUT a timezone, which means doing CONVERT_TIMEZONE('MST', time) is
      # telling the database "hey!  This timestamp is already in MST, please tack
      # a timezone on it and convert it to UTC". That means Snowflake will ADD
      # seven hours, not subtract. UGH.

      # Okay. Okay. Okay. I found a thing that works.
      # First, we need to send "ALTER SESSION SET TIMEZONE = 'UTC' as often as we
      # need to. I'm guessing every script should start off that way.

      # TEST THIS: It might be DataGrip that was messing that up by "helpfully"
      # reading my laptop's local settings. So when we run a script, it might
      # already be in UTC. Nope. Maybe? The server appears to be in the US/Los
      # Angeles time zone, which is PT with DST. UGH.

      # Some cases, maybe. E.g. field ATZ 'MST' ATZ 'EST'
      # would convert to CONVERT_TIMEZONE('EST', 'MST', field)
      detect_and_print_lines lines, "AT TIME ZONE", /\bAT TIME ZONE\b/i, essential: true, suggest: :replace_at_time_zone
    end

    # ----------------------------------------------------------------------
    # TODO: CREATE TABLE ( LIKE )
    # ----------------------------------------------------------------------
    # change CREATE TABLE foo ( LIKE bar ) -> CREATE TABLE foo LIKE bar
    #
    # Needs multiline search to find "create table" on one line and "( like" on
    # the next
    #
    # Originally found in etl_f_delighted_survey_responses.py
    # ----------------------------------------------------------------------
    # detect_and_print_lines lines, "name?", finder: :find_create_table_foo_paren_like_bar

    # ----------------------------------------------------------------------
    # UNLOAD
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "UNLOAD", /\bUNLOAD\b\s*\(/i, essential: true

    if run_trunc_detectors?
      # ----------------------------------------------------------------------
      # TRUNC
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "TRUNC (Check by hand, must have 2 args)", /\bTRUNC\b\s*\(/i, warning: true
    end

    # ----------------------------------------------------------------------
    # SIMILAR TO -> RLIKE / LIKE ANY
    #
    # RED: <column> SIMILAR TO <regex>
    # SNOW: RLIKE(<column>, <regex>)
    #
    # NOTE: For simple expressions favor LIKE ANY ('%bleh%', '%glorp%')
    # Use RLIKE when the regexp is genuinely complicated
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "SIMILAR TO", /(similar to|~)/i, suggest: :replace_similar_to_with_like_any, unmatch: /os\.path\.expanduser/

    if run_json_detectors?
      # ----------------------------------------------------------------------
      # JSON_EXTRACT_* functions
      #
      # 2022-11-21: Per Zack, consider replacing with PARSE_JSON and handling the
      # resulting variant instead. DS-1746 came out much cleaner that way:
      #
      # Strict Replacement: JSON_EXTRACT_PATH_TEXT(json, '[' || level || ']')
      # Cleaner Replacement: PARSE_JSON(json)[level]
      #
      # JSON_EXTRACT_PATH_TEXT - Path may need tweaking but should port fine as
      # long as it only has 2 or 3 arguments and if the third argument is present,
      # it is TRUE. (Redshift allows TRUE/FALSE, Snowflake only does the TRUE
      # version. Our codebase only does TRUE to the best of my knowledge).
      #
      # JSON_EXTRACT_ARRAY_ELEMENT_TEXT - This can be expressed as a
      # JSON_EXTRACT_PATH_TEXT call:
      #
      # JEAET('json_array', 3) -> JEPT('json_array', '[3]')
      detect_and_print_lines lines, "JSON MANIPULATION FUNCTIONS",
                             %r{JSON_EXTRACT_PATH_TEXT|JSON_EXTRACT_ARRAY_ELEMENT_TEXT|IS_VALID_JSON|IS_VALID_JSON_ARRAY|JSON_ARRAY_LENGTH|JSON_EXTRACT_ARRAY_ELEMENT_TEXT|JSON_PARSE|CAN_JSON_PARSE|JSON_SERIALIZE|JSON_SERIALIZE_TO_VARBYTE},
                             essential: true
      # , unmatch: /JSON_EXTRACT_PATH_TEXT\([^,]+,[^,]+\)/ # JEPT/2 good; JEPT/3 bad.
    end

    if run_quotes_detectors?
      detect_and_print_lines lines, "Doubled Quotes (Check By Hand)", /''.*''/, warning: true

      detect_and_print_lines lines, ".replace(\"'\", \"''\") quotes with doubled quotes", /\.replace[\(\)\,\s\'\"]+$/, warning: true
    end


    if run_date_detectors?
      # ----------------------------------------------------------------------
      # DATE_ADD('unit',num, column) -> DATEADD(unit, num, column)
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "DATE_ADD", /DATE_ADD\('\w+'/i, autofix: %Q|sed -i -E "s/DATE_ADD\\('([^']+)'/DATEADD(\\1/ig" #{get_script}|

      # ----------------------------------------------------------------------
      # DATE_DIFF('unit',num, column) -> DATEDIFF(unit, num, column)
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "DATE_DIFF", /DATE_DIFF\('\w+'/i, autofix: %Q|sed -i -E "s/DATE_DIFF\\('([^']+)'/DATEDIFF(\\1/ig" #{get_script}|

      # ----------------------------------------------------------------------
      # DATEPART -> DATE_PART
      #
      # YES, this is the OPPOSITE of date add and diff: Redshift does NOT have an
      # underscore and snowflake requires one. At least the units are the same;
      # Redshift actually uses bareword identifiers here. :shrug:
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "DATEPART", /\bDATEPART\b/i, autofix: %Q|sed -i -E "s/\\bDATEPART\\b/DATE_PART/ig" #{get_script}|
    end

    # ----------------------------------------------------------------------
    # BTRIM
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "BTRIM", /\bBTRIM\(/i, autofix: "sed -i -E 's/BTRIM\\(/TRIM\\(/g' #{get_script}"

    # ----------------------------------------------------------------------
    # TO_CHAR
    #
    # You can specify a fixed char size for a numeric field with 9's. Redshift
    # allows this to be an integer. Snowflake demands a string:
    #
    # Red:
    # SELECT '[' || TO_CHAR(99, 999) || ']'       # [ 99]
    # SELECT '[' || TO_CHAR(9999, 999) || ']'     # [###]
    #
    # Sno:
    # SELECT '[' || TO_CHAR(99, '999') || ']'     # [ 99]
    # SELECT '[' || TO_CHAR(9999, '999') || ']'   # [###]
    #
    # match will unmatch if there is a last argument in quotes that's like
    # 999,999D99 or YYYY-MM-DD etc.
    # ----------------------------------------------------------------------
    if run_to_char_detectors?
      detect_and_print_lines lines, "TO_CHAR (with numeric format)", /to_char/i, unmatch: /'[YMD\-9,]+'/
    end

    # ----------------------------------------------------------------------
    # WITH NO SCHEMA BINDING -> just delete the line
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "WITH NO SCHEMA BINDING", /WITH NO SCHEMA BINDING/i

    # ----------------------------------------------------------------------
    # BOOL -> BOOLEAN
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "BOOL", /\bBOOL\b/i, autofix: "(!!!BEWARE!!! THIS WILL FALSE-TARGET bool(x) IN PYTHON CODE) sed -i -E 's/\\bBOOL\\b/BOOLEAN/ig' #{get_script}"

    if run_days_detectors?
      # ----------------------------------------------------------------------
      # [WITH] days AS
      #
      # You can have a CTE named days in redshift, but in snowflake it's a
      # keyword.
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "DAYS AS", /\bdays\s+as\b/i, essential: true
    end

    # ======================================================================
    # END SQL CHANGES
    # ======================================================================

    # ----------------------------------------------------------------------
    # NPM-style Lists
    # ----------------------------------------------------------------------
    # I deeply dislike NPM-style leading-commas. Casey loves them. Be kind to
    # the maintainer. If there's a motherhuge query that only Casey can debug,
    # or the entire file is consistently in NPM-style, leave it in his preferred
    # style. If there's a mix of styles or it's something I have to maintain,
    # change it. Unless there's already a good reason to be editing a query,
    # don't change its style.
    detect_and_print_lines lines, "NPM-style Lists (Leading Commas)", /^\s*,\s*\S+/, disabled: true

    # ----------------------------------------------------------------------
    # Redshift S3 buckets
    # These will probably stay in the project until/unless we can find out who
    # is reading from these s3 buckets.
    # ----------------------------------------------------------------------
    if run_python_detectors?
      detect_and_print_lines lines, "Redshift S3 Buckets", %r|s3.*redshift|i, essential: true
    end

    # ----------------------------------------------------------------------
    # Redshift mentioned anywhere
    # ----------------------------------------------------------------------
    # (<!s3) is "negative lookbehind". It means find redshift, but not if it was
    # preceded by s3 (which is found by Redshift S3 buckets above)
    detect_and_print_lines lines, "Redshift mentioned anywhere", /redshift/i, unmatch: %r|s3.*redshift|

    # ----------------------------------------------------------------------
    # Secretly blocked by DS-1704: d_team_list_rac_cs_gsheet
    # ----------------------------------------------------------------------
    # detect_and_print_lines lines, "Secretly blocked by DS-1704", %r|D_TEAM_LIST_RAC_CS_GSHEET|i, essential: true

    # ----------------------------------------------------------------------
    # Update Python String Formatting
    # ----------------------------------------------------------------------
    if run_python2_detectors? && run_python_detectors?
      detect_and_print_lines lines, "Python 2 String Formatting", %r|(?<!%)%[\d\.]*[sdf].*|, unmatch: %r{(str[pf]time|%Y-%m-%d|LIKE ANY)}
    end

    # ----------------------------------------------------------------------
    # os.chdir to self: remove
    # ----------------------------------------------------------------------
    # If we see this exact line, just remove it:
    # os.chdir(os.path.dirname(os.path.abspath(__file__)))
    #
    # If we see another os.chdir, flag it as a warning.
    # the autofix here is sed -E -i 's/\bos.chdir\b/d'
    if run_python_detectors?
      detect_and_print_lines lines, "Potentially Useless os.chdir", /\bos.chdir\b/, autofix: "sed -E -i '/\\bos.chdir\\b/d' #{get_script}"
    end

    # ----------------------------------------------------------------------
    # Excessive globals in main
    # ----------------------------------------------------------------------
    # Needs custom finder. Count up lines after def main that contain "global"

    # ----------------------------------------------------------------------
    # Mutual globals
    # ----------------------------------------------------------------------
    # Look for globals in other methods. Don't overautoanalyze the code (not
    # worth the time to write it) but maybe a note of "here we have global
    # start_date, see where it is used and maybe upgrade it to a parameter
    # instead of a global".

    if run_exception_detectors? && run_python_detectors?
      # ----------------------------------------------------------------------
      # except Exception
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "Global Exception Handling", /\bexcept Exception\b/, warning: true
    end

    # ----------------------------------------------------------------------
    # job_config must not be optional
    # ----------------------------------------------------------------------
    # this is a sweeping grep, should work 95% of the time. SOME false positives
    # will occur that COULD be avoided by checking etl_nightly for the
    # run_method and grepping only for THAT method but that will probably take
    # more time than just weeding the falses out by hand. Looks like there are
    # only 2 that take config=None while all the other true positives are
    # job_config=None, so.
    if run_python_detectors?
      detect_and_print_lines lines, "Optional job_config", /def (run|main)\(.*config=/
    end

    # ----------------------------------------------------------------------
    # testing for job_config == None or job_config is None
    # ----------------------------------------------------------------------
    if run_python_detectors?
      detect_and_print_lines lines, "Testing for presence of job_config", /\bjob_config\s+(is|==)\s+None\b/
    end

    # ----------------------------------------------------------------------
    # except Exception:
    # ----------------------------------------------------------------------
    # if run_python_detectors?
    #   detect_and_print_lines lines, "except Exception:", /except Exception:/, autofix: "sed -E -i 's/except Exception:/except:/' #{get_script}"
    # end

    # ----------------------------------------------------------------------
    # unsize varchars
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Unsize VARCHAR", /varchar\(\d+\)/i, autofix: %Q|ruby -i -n -e 'puts $_.gsub(/varchar\\(\\d+\\)/, "varchar").gsub(/VARCHAR\\(\\d+\\)/, "VARCHAR")' #{get_script}|

    # ----------------------------------------------------------------------
    # TODO: no-op reraise in exception
    # marketing_device_traffic.py
    #
    # <blank line(s)>
    # except.*:
    #     raise
    #
    # Fix: delete those lines
    # ----------------------------------------------------------------------

    # ----------------------------------------------------------------------
    # case and paste help messages for arparse
    # search for different options but same help messages, e.g.
    #     parser.add_argument('--files', help='Start Date', required=True)
    #     parser.add_argument('--start_date', help='Start Date', required=False)
    #     parser.add_argument('--end_date', help='Start Date', required=False)
    # ----------------------------------------------------------------------


    # ----------------------------------------------------------------------
    # TODO: etl_client.close() before finally
    # Actually, there's a whole common thing here:
    #
    # 1. Detection:
    #   1. Find the 'except Exception[ as e]:' line
    #   2. Is it followed only by 1 or more of etl_client.close(), dbo.closue, or raise
    #   3. It MAY be preceded by 1 or 2 blank lines
    #   4. Those blank lines may by preceded by another etl_client.close() (and
    #   potentially dbo.close(), but that's super rare, so ignore and faster to
    #   fix it by hand when it occurs)
    #
    # 2. Correction:
    #   1. Delete the preceding etl close
    #   2. Insert 1 blank line between that and the finally
    #   3. Change the except Exception line to finally, keeping same indent
    #   4. Delete the raise
    # ----------------------------------------------------------------------

    # ----------------------------------------------------------------------
    # main() bootstrapper
    # ----------------------------------------------------------------------
    if run_python_detectors?
      detect_and_print_lines lines, "main() bootstrapper", /if\s*__name__\s*==\s*["']__main__["']\s*:/
    end

    if run_missing_detectors? && run_python_detectors?
      detect_missing_skip_upload
      detect_missing_test_mode if run_test_detectors?
      detect_missing_recipients_override
    end

    # ----------------------------------------------------------------------
    # CREATE TABLE a (LIKE b) -> CREATE TABLE a LIKE b
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "CREATE TABLE LIKE [1/3]", /\bCREATE\s+TABLE\b.*\(LIKE/i, suggest: :replace_inline_create_table_like, essential: true
    detect_and_print_multilines lines, "CREATE TABLE LIKE [2/3]", [/\bCREATE\s+TABLE\b.*\(/i, /LIKE/i], essential: true
    detect_and_print_multilines lines, "CREATE TABLE LIKE [3/3]", [/\bCREATE\s+TABLE\b/i, /\(\s*LIKE/i], essential: true

    # ----------------------------------------------------------------------
    # ENCODE
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "ENCODE", /\bencode\s+\S+/i, essential: true

    # ----------------------------------------------------------------------
    # GREATEST/LEAST -> COALESCE(GREATEST/LEAST(...),value)
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "GREATEST/LEAST", /\b(greatest|least)\b/i, essential: true # , unmatch: /coalesce\((greatest|least)\b\(/i

    # ----------------------------------------------------------------------
    # COALESCE/1
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "COALESCE/1", /coalesce\([^\(,]+\)/i, essential: true

    if run_whitespace_detectors?
      # ----------------------------------------------------------------------
      # TABS
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "TABS", /\t/
    end

    # If we're doing quiet mode but stdout isn't being piped, print "OK" or "Errors detected"
    if quiet? && $stdout.tty?
      puts seen_errors? ? "Errors detected".red : "OK".green
    end

    # return error status, if we saw any errors
    !seen_errors?
  end
end


if __FILE__ == $0
  success = SnowflakeManglerApp.new.run

  exit(-1) unless success
end
