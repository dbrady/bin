#!/usr/bin/env ruby
# snow - Command suite to support moving to snowflake
#
# DONE: snow -q, --quick emits only the detectors that fail, so you can see
# remaining work
#
# TODO: port this to Optimist*. The arg processing is killing me here
#
# TODO: instead of catching individual commands, take ARGV.first and check for
# an executable "snow-<arg>" script in bin
#
# TODO: Document the snow ecosystem for future me:
#
# TODO: Keep the job name around when off of the branch. Now we have the ability
# to blast a flash warning on the screen saying "you're off-branch, are you
# sure?". That would be more convenient that running python manually--and losing
# the checks in the process.
#
# TODO: Write fixes to a script file instead of just the console. Hand-copying
# 40 sed commonds to fix timezones has the sheets all the way up my crack.
#
# - snow - check the current script. -q to only show failing checks.
# - snow main - jump back to main snowflake branch
# - snow master - same thing but back to origin/master
# - snow new <id> <file> - create new branches
# - snow run
# - snow-commit-warehouse-* will commit the script with a specific message. It
#     does not modify the script.
# - scow <term> will show you all snow-commit-warehouse-*<term>* files, let you
#     pick one with selecta, then run it.
# - snow-fix-* will modify the script AND commit it.

require 'colorize'
require 'optimist'
require 'io/console'

class SnowflakeManglerApp
  attr_reader :opts
  attr_accessor :argv

  def checklist
    run_and_exit! "snow-show-checklist #{argv.join(' ')}"
  end

  def set_script(script)
    @get_script = script
  end

  def get_script
    return @get_script if @get_script

    @get_script = `get-ds-script`.strip
    @get_script = ENV['SCRIPT'] if @get_script.empty?
    @get_script
  end

  # Some commands do not need the script var to be set; in fact they need to be
  # able to when it is not set (because they cause it to become set)
  def need_script?
    no_need = (["backlog", "go", "link", "new", "master", "main", "cplx", "complexity"] & argv).size > 0 ||
              (argv.first == "job" && argv.size >= 2)
    !no_need
  end

  def osx?
    `uname -s`.strip == "Darwin"
  end

  def docker?
    ENV['IS_DOCKER_LOCAL'] = "1"
  end

  # ----------------------------------------------------------------------
  # OPTIONS ACCESSORS - Because this script made it 6 months and 1100 LOC before
  # I added the Optimist gem and by then it was detecting arguments by Doing Its
  # Own Thing Man(TM)
  # ----------------------------------------------------------------------

  # force: proceed in the face of warnings
  def force?
    opts[:force]
  end

  # Quick: Don't show passing detectors or extraneous logging
  def quick?
    opts[:quick]
  end

  # For job config only: show the default job config AND this job config (to show complete config)
  def defaults?
    opts[:defaults]
  end

  # Was this script invoked with --essential?
  def essential?
    opts[:essential]
  end

  # Was this script invoked with --list?
  def list?
    opts[:list]
  end

  def preview?
    argv.include?("preview")
  end

  def job?
    argv.include?("job")
  end

  def run_timezone_detectors?
    opts[:tz]
  end

  def run_python2_detectors?
    opts[:python2]
  end

  def run_json_detectors?
    opts[:json]
  end

  def run_missing_detectors?
    opts[:missing]
  end

  def run_whitespace_detectors?
    opts[:space]
  end

  def run_quotes_detectors?
    opts[:quotes]
  end

  def run_date_detectors?
    opts[:dates]
    # Should I maybe do opts[:disable].split(/,/).include?("date")
    # So I can say --disable=date,json etc?
    #
    # Right now I kinda like --no-json --no-dates, so eh. But if it tuns into 20
    # different --no-foo options, this might be the way.
  end

  def run_exception_detectors?
    opts[:exceptions]
  end

  def use_test_mode?
    opts[:test]
  end

  # ----------------------------------------------------------------------
  # END OPTIONS ACCESSORS
  # ----------------------------------------------------------------------

  def section_title(title)
    "====%s" % " #{title} ".ljust(80, "=").cyan
  end

  def log_ok
    return if quick?
    puts " OK ".bold.light_white.on_green
  end

  # ======================================================================
  # DETECTORS
  # ======================================================================

  # This is turning into a bit of a swiss army kitchen sink...
  # lines:     array of lines to check
  # title:     section title to print if matches are found
  # regex:     use this to detect matching lines. If regex is an array of regexes,
  #            finds matches on adjacent lines
  # disabled:  this check is turned off. Do not run, even if marked essential
  # essential: this check is essential. If we're running snow -e, skip
  #            inessential checks
  # autofix:   text containing an autofix. Usually a bash command applied to the
  #            whole file
  # suggest:   a method that receives the matching line, and returns a suggested
  #            fix.
  # unmatch:   ignore lines that match regex but ALSO match this regex.
  # warning:   Warning only and/or the detector will continue to fire even after
  #            the problem has been fixed
  def detect_and_print_lines(
        lines,
        title,
        regex,
        essential: false,
        disabled: false,
        autofix: nil,
        suggest: nil,
        unmatch: nil,
        warning: nil
      )
    return if disabled
    return if essential? && !essential
    puts section_title title unless quick?
    # OOF.
    # match the regex AND (we have no unmatch, or line is not unmatched)
    if lines.any? { |line| line =~ regex && (!unmatch || line !~ unmatch) }
      puts section_title title if quick?
      if autofix
        puts "Autofix with:\n#{autofix.cyan}"
        fixlog "#{get_script}: autofix #{title}", autofix
      end

      # Yeah, turn this into lines_with_indexes = finder_method, we can
      # write a generic version to do what this regex does, and then we can
      # pass in custom finder methods for multiline and trickier stuff
      lines.each.with_index do |line, index|
        if line =~ regex && (!unmatch || line !~ unmatch)
          puts "%4d: %s" % [index+1, line]
          if suggest && suggestion = send(suggest, line)
            puts "Replacement:"
            puts line.red
            puts suggestion.green
            sed_replacement = suggestion.gsub(%r|/|, "\\/").gsub('"', '\"')

            sed_command = "sed -i -E \"#{index+1}s/^.*$/#{sed_replacement}/\" #{get_script}"
            puts sed_command.cyan
            fixlog "#{get_script}: replacement for #{title}", sed_command
          end
        end
      end
    else
      log_ok
    end
  end

  def detect_and_print_multilines(
        lines,
        title,
        regexes,
        essential: false,
        disabled: false,
        autofix: nil,
        suggest: nil,
        unmatch: nil
      )
    return if disabled
    return if essential? && !essential
    puts section_title title unless quick?

    # given n regexes, at line[m], then...
    # Simple Match For Now:
    # 1. lines[m..m+n].zip(regexes).all? {|line, regex| line =~ regex }
    #
    # More complex matching might allows for same-line stuff, e.g. if we are
    # matching on [/create/, /table/, /like/], we should match if:
    #
    # (line[m] =~ /create.*table.*like/) ||
    # (line[n] =~ /create/ && line[n+1] =~ /table.*like/) ||
    # (line[n] =~ /create.*table/ && line[n+1] =~ /like/) ||
    # (line[n] =~ /create/ && line[n+1] =~ /table/ && line[n+2] =~ /like/)
    no_matches = true
    first = true
    0.upto(lines.size - regexes.size) do |main_offset|
      if (0...regexes.size).map {|j| [lines[main_offset+j], regexes[j]] }.all? {|line, regex| line =~ regex }
        no_matches = false
        if first
          first = false
        else
          puts "--"
        end
        puts (0...regexes.size).map {|j| "#{main_offset+j}: #{lines[main_offset+j]}" }
      end
    end

    log_ok if no_matches
  end

  # Let's start capturing the fixes and suggests to a script/log
  def fixlog(comment, autofix)
    comment = comment.gsub(%r|sql/|, '')
    autofix = autofix.gsub(%r|sql/|, '')

    autofix_filename = File.expand_path("./autofixes/#{get_script}.sh")
    if !File.exists?(autofix_filename)
      File.open(autofix_filename, "w") { |file| file.puts "#!/bin/bash"}
    end
    File.open(autofix_filename, "a") { |file| file.puts "# #{comment}"; file.puts autofix}
  end

  def detect_missing_skip_upload
    puts section_title "Skip Upload" unless quick?
    if should_have_skip_upload? && !has_skip_upload?
      puts section_title "Skip Upload" if quick?
      puts "File is mccloud or salesforce but does not have --skip_upload option"
    else
      log_ok
    end
  end

  def detect_missing_test_mode
    puts section_title "--test" unless quick?
    if should_have_test_mode? && !has_test_mode?
      puts section_title "--test" if quick?
      puts "File mentions portal, gsheet, facebook, or commissions but does not have --test option"
    else
      log_ok
    end
  end

  def detect_missing_recipients_override
    if should_have_recipients_override? && !has_recipients_override?
      puts section_title "--recipients" if quick?
      puts "File sends email but does not have --recipients override option"
    else
      log_ok
    end
  end

  def detect_potentially_unmigrated_tables(lines, essential: false, disabled: false)
    return
    zack_casey_tables = %w(
      f_collections_chargeoffs
    )
    # {yellow: diy_tables, red: zack_casey_tables}.each do |color, tables|
      {red: zack_casey_tables}.each do |color, tables|
      regex = /(#{tables.join('|')})/

      puts section_title "Potentially Unmigrated Tables (#{color})" unless quick?
      if lines.any? {|line| line =~ regex }
        puts section_title "Potentially Unmigrated Tables (#{color})" if quick?
        lines.each.with_index do |line, index|
          problem_tables = tables.find_all {|table| line =~ /\b#{table}\b/ }
          next if problem_tables.size.zero?

          problem_tables.each do |table|
            line = line.gsub(/#{table}/, table.send(color))
          end
          puts "%4d: %s" % [index+1, line]
        end
      else
        log_ok
      end
    end
  end

  def detect_unmanaged_whitespace(lines)
    puts section_title "UNMANAGED WHITESPACE" unless quick?
    if lines.any? {|line| line =~ /\s+$/}
      puts section_title "Unmanaged whitespace" if quick?
      lines.each.with_index do |line, index|
        if line =~ /(\s+)$/
          colorized_line = line.sub(/\s+$/, $1.on_red)
          puts "%d: %s" % [index+1, colorized_line]
        end
      end
    else
      log_ok
    end
  end

  def skip_pep_check?
    true
  end

  # ======================================================================
  # END DETECTORS
  # ======================================================================

  def has_skip_upload?
    IO.readlines(get_script).any? { |line| line.include?("parser.add_argument('--skip_upload") }
  end

  def should_have_skip_upload?
    get_script.include?("_sf_") ||
      get_script.include?("salesforce") ||
      (get_script.include?("mcloud") && !get_script.include?("_get_"))
  end

  def has_test_mode?
    File.readlines(get_script).any? {|line| line.include?("parser.add_argument('--test")}
  end

  def has_start_date?
    File.readlines(get_script).any? {|line| line.include?("parser.add_argument('--start_date")}
  end

  def has_end_date?
    File.readlines(get_script).any? {|line| line.include?("parser.add_argument('--end_date")}
  end

  def should_have_test_mode?
    # this is just a tiny tiny sample
    ["gsheet", "facebook", "commissions"].any? {|part| get_script.include?(part) } ||
      File.readlines(get_script).any? {|line| line.include? "portal" }
  end

  def has_recipients_override?
    File.readlines(get_script).any? {|line| line.include? "parser.add_argument('--recipients'" }
  end

  def should_have_recipients_override?
    # true if we see "mail_recipients" anywhere in the file
    # -OR- if we see "mail_recipients" in the job config
    File.readlines(get_script).any? {|line| line.include? "mail_recipients" }
  end

  def run_and_exit!(command)
    system command
    exit $?.exitstatus
  end

  def usage
    str = <<~USAGE
snow - master control script for snowflake job migration

snow [<command> [<options>]]

Most commands use get-ds-script, and/or check the last line of ~/.ds_script for
the name of the script filename. For legacy reasons this is called the SCRIPT
file. It should point to the job file relative from the warehouse
directory. E.g. for ~/dataservices/etl/warehouse/rac/rac_job1.py, do
set-ds-script rac/rac_job1.py

Many jobs check that the current branch matches the SCRIPT file. This puts an
extra bit of bookkeeping on you, but it's actually there to ensure that you're
not accidentally merging to master or snowflake-moveover. Again.

Commands/options:
check [-e|--essential] [job.py]  Check the [current] job for [only essential] coding violations
job [job]                        Show current [or specified] job's configuration
job s[nowflake]                  Convert job configuration to snowflake
job r[edshift]                   Convert job configuration back to redshift
run                              Run the current job
main                             Checkout the snowflake main branch in both repos
master                           Checkout master in both repos
go <ticket>|<job_name>           Checkout the branch containing this search term
new <ticket> <job.py>            Create a new feature branch in both repos
link <pr> <finance_pr> [pkg_pr]  Create links template for Zack
slam                             Commit finance config change and open PRs for both repos

If no commands are given, snow will run check.

Options:
    USAGE
    # actual options are automagically presented by Optimist after displaying this banner text
  end

  def replace_at_time_zone(line)
    # AND t.contact_start AT TIME ZONE 'UTC' AT TIME ZONE 'America/Denver' >= '2021-01-23'
    # ,MIN(v.created_at) AT TIME ZONE 'UTC' AT TIME ZONE 'America/Denver' AS awaiting_delivery_at_local
    #
    # in a subselect with double quotes and doubled single quotes
    # ,cc.updated_at AT TIME ZONE ''UTC'' AT TIME ZONE ''America/Denver'' AS "Updated Date"

    # My nex regex chokes on this special case which is uncommon but not rare
    getdate_regex = %r|^(.*)GETDATE\(\) AT TIME ZONE 'UTC' AT TIME ZONE 'America/Denver'(.*)$|i
    if line =~ getdate_regex
      puts "MATCH!!!".white.bold.on_green
      return line.sub(getdate_regex, %q|\1CONVERT_TIMEZONE('UTC', 'America/Denver', GETDATE())\2|)
    else
      puts "NO MATCH".white.bold.on_red
    end

    regex = /^(\s*,?)([^,]+) AT TIME ZONE ('+[^']+'+) AT TIME ZONE ('+[^']+'+)/i
    #           ^      ^                     ^                          ^
    #           |      |                     |                           \_ $4. "to" time zone
    #           |      |                      \_ $3. "from" time zone
    #           |       \_ $2. timestamp column to be converted
    #            \_ $1. indentation and optional leading comma
    #

    return unless line =~ regex

    # line.gsub(regex, "#{$1}CONVERT_TIMEZONE(#{$3}, #{$4}, #{$2})")
    line = line.gsub(regex, '\1CONVERT_TIMEZONE(\3, \4, \2)')

    # Sometimes we get extra parens around the timestamp, like so:
    # CONVERT_BLAH(x, y, (c.contact_start))

    # This is tricky to catch going in because the regex breaks $2 right before
    # the closing paren. However, these are detectable after the replacement
    # because the innermost match has no whitespace or parens.
    line.gsub( /\(([A-Za-z0-9_\.]+)\)/, '\1' )
  end

  # This doesn't get all the cases, but it does get the ones that are inline.
  def replace_inline_create_table_like(line)
    regex = /\(LIKE (.+?)\)/
    return unless line =~ regex

    line.gsub(regex, "LIKE #{$1}")
  end

  def replace_epoch_timestamp_with_interval_1_second(line)
    regex = /timestamp\s+'epoch'\s*\+\s*(.*?)\*\s*interval\s+'1 second'/i

    return unless line =~ regex

    replacement = $1.strip
    replacement = replacement[1..] if replacement.start_with? "("
    replacement = replacement[..-2] if replacement.end_with? ")"
    replacement = replacement.strip

    replacement = "TO_TIMESTAMP(#{replacement})"

    line.gsub(regex, replacement)
  end

  def replace_similar_to_with_like_any(line)
    regex = %r{^(.*?)([A-Za-z0-9\._]+)\s+((NOT)\s+)SIMILAR\s+TO\s+(.+)}
    return unless line =~ regex

    # column, is_not, patterns = $1, $3, $4
    left, column, is_not, patterns = $1, $2, $4, $5

    replacement = "column: #{column}, not: #{is_not ? 'NOT!' : 'not-not'}, patterns: #{patterns.inspect}"

    pats = if patterns =~ /^'%\((.*?)\)%'$/
             $1.split(/\|/).map {|pat| "'%#{pat}%'"}.join(', ')
           else
             "NO MATCH"
           end


    "#{left}#{is_not ? 'NOT ' : ''}#{column} LIKE ANY (#{pats})"

    # x SIMILAR TO '%(pants)%' => x LIKE '%pants%'
    # x SIMILAR TO '%%(pants)%%' => x LIKE '%pants%'
    # x SIMILAR TO '%(hat|glasses)%' => x LIKE ANY ('%hat%', '%glasses%')

    # To negate, put the NOT in front of the column name
    # x NOT SIMILAR TO '%(pants)%' => NOT x LIKE ANY ('%pants%')

    # match_phrase = \
    # if match begins and ends with %(...)% and contains no other %'s
    #   strip leading %( and trailing )%
    #   # is match plural?
    #   matches = match.split(%r|/|).map {|m| "'%#{m}%'" }.join(", ")
    #   "LIKE ANY (#{matches})"
    #
    # else
    #   it's a weird match, let the human do it

    # line
    #   .sub(regex, "RLIKE(#{$2})")
    #   .sub(%r{'%\(?}, "'.*(")
    #   .sub(%r{\)?%'}, ").*'")
    #   .gsub(/\+/, '\\\\+')
  end


  # THE MAIN APP METHOD, LET'S GO
  def run
    banner_text=usage # bc we can't call self.usage inside this do...end (it changes self)
    @opts = Optimist.options do
      banner banner_text

      @ignore_invalid_options = true

      opt :force, "Force continuation over warnings, such as get-ds-script not pointing at an actual file. Also send -f to git checkout (for/if submodule errors)"
      opt :quick, "Only show output from failing detectors"
      opt :debug, "Output extra debug info"
      opt :essential, "Only run essential detectors"
      opt :list, "Show PR workflow checklist"
      opt :defaults, "(job only) Include job config defaults with job config (to see complete config)"
      opt :exception, "Run exception detectors", default: true
      opt :json, "Run JSON detectors", short: :none, default: true
      opt :missing, "Run missing detectors", short: :none, default: true
      opt :quote, "Run double-quote detectors", default: true
      opt :timezone, "Run time zone detectors", default: true
      opt :python2, "Run python2 string formatting detectors", default: true
      opt :test, "Send --test mode to jobs that offer it", short: :none, default: true
      opt :"no-2", "same as --no-python2", short: :none, default: false
      opt :"no-j", "same as --no-json", short: :none, default: false
      opt :"no-m", "same as --no-missing", short: :none, default: false
      opt :"no-q", "same as --no-quotes", short: :none, default: false
      opt :"no-s", "same as --no-space", short: :none, default: false
      opt :"no-t", "same as --no-timezone", short: :none, default: false
      opt :"no-x", "same as --no-exception", short: :none, default: false
      opt :space, "Run unmanaged whitespace detectors", default: true
      opt :wat, "Dump options and exit", default: false
    end
    # copy alternate args over
    {
      exception: "x",
      json: "j",
      missing: "m",
      quote: "q",
      space: "s",
      timezone: "t",
      python2: "2"
    }.each do |option, shortcode|
      # opts[:json] = opts[:"no-j"] if opts[:"no-j_given"]
      opts[option] = opts[:"no-#{shortcode}"] if opts[:"no-#{shortcode}_given"]
    end

    puts opts.inspect if opts[:debug]
    @argv = ARGV.dup

    # Optimist::die :dynos, "must be a non-negative number" unless opts[:dynos] >= 0.0

    # if -l or --list, show checklist and exit
    if list?
      checklist
      # puts "HIT ENTER TO EXIT"
      # $stdin.gets
      exit 0
    end

    # Override script if job is preview
    if preview?
      # If arg 1 is an integer, we've been given a ticket number also
      set_script((argv[1].to_i > 0) ? argv[2] : argv[1])
    end

    # Optionally override script if job is show config for job
    if job?
      # If arg 2 exists and is a file that exists, override it
      if argv[2] && File.exists?(argv[2])
        set_script(argv[2])
      end
    end

    # 2.2 If you do not have SCRIPT set, I tell you about SCRIPT and exit
    if need_script? && (get_script.nil? || get_script.empty?)
      puts "You must run set-ds-script <path/to/job.py> first."
      usage
      exit -1
    end

    # ----------------------------------------------------------------------
    # 3. dispatch/handle commands
    # ----------------------------------------------------------------------

    # ----------------------------------------------------------------------
    # snow run -> dsetl_nightly
    # dsetl_nightly reads SCRIPT from rc file; Use dsetl to specify from CLI
    # ----------------------------------------------------------------------
    if argv.first == "run"
      if !docker?
        puts "This script must be run from inside docker.".bold.red
        exit -1
      end

      # Check to see if etl_nightly.py has been gorbled by git, which has
      # happened twice in the last week.
      #
      # "If I had a nickel for every time [git has trashed etl_nightly.py], I'd
      # have two nickels - which isn't a lot, but it's weird that it happened
      # twice."
      if !system("grep -- '--retry_count' etl_nightly.py > /dev/null")
        puts "etl_nightly.py has no mention of the retry_count parameter. Is the finance submodule out of sync?".bold.white.on_red
        puts
      end

      # print dst_conn warehouse name in top-right corner
      system("dsannouncewarehouse") unless quick?

      # TODO: I would like to add -c to 'snow run' to "show command only",
      # but the showing of the command is entirely inside dsetl_nightly. So we
      # generate all the arguments here, but we display them over there. This is
      # the opposite of SRP--a single concern has been separated into two
      # separate programs.
      argv.shift # clip "run" off of the argv
      command = "dsetl_nightly"
      command += " --skip_upload" if has_skip_upload?
      command += " --force" if force?
      command += " --test" if has_test_mode? && use_test_mode?
      command += " --start_date #{(Date.today - 60).strftime('%F')}" if has_start_date? && !argv.include?('--start_date')
      command += " --end_date #{(Date.today - 59).strftime('%F')}" if has_end_date? && !argv.include?('--end_date')
      command += " --recipients david.brady@acima.com" if has_recipients_override? && !argv.include?('--recipients')
      command += " " + argv.join(" ")
      run_and_exit! command
    end

    # ----------------------------------------------------------------------
    # snow main -> snow-go-main
    # ----------------------------------------------------------------------
    if argv.first == "main"
      run_and_exit! "snow-go-main"
    end

    # ----------------------------------------------------------------------
    # snow master -> snow-go-master
    # ----------------------------------------------------------------------
    if argv.first == "master"
      run_and_exit! "snow-go-master"
    end

    # ----------------------------------------------------------------------
    # snow go <id> -> snow-go-branch
    # ----------------------------------------------------------------------
    if argv.first == "go"
      run_and_exit! "snow-go-branch #{force? ? "-f " : ""}#{argv[1..] * ' '}"
    end

    # ----------------------------------------------------------------------
    # snow rebase [parent_branch] -> snow-rebase
    # ----------------------------------------------------------------------
    if argv.first == "rebase"
      run_and_exit! "snow-rebase #{argv[1..] * ' '}"
    end

    # ----------------------------------------------------------------------
    # snow cplx | complexity
    # ----------------------------------------------------------------------
    if (argv & ["cplx", "complexity"]).size > 0
      run_and_exit! "snow-complexity #{argv[1..] * ' '}"
    end

    # ----------------------------------------------------------------------
    # snow job -> snow-show-job-config
    # ----------------------------------------------------------------------
    if argv.first == "job"
      if defaults?
        @argv = argv.dup - ["defaults", "d", "--defaults"]
        run_and_exit! "snow-job-show-config --defaults #{argv[1..].join(' ')}"
      elsif argv[1] == "redshift" || argv[1] == "r"
        run_and_exit! "snow-job-convert-to-redshift #{get_script}"
      elsif argv[1] == "snowflake" || argv[1] == "s"
        run_and_exit! "snow-job-convert-to-snowflake #{get_script}"
      else
        run_and_exit! "snow-job-show-config #{argv[1..].join(' ')}"
      end
    end

    # ----------------------------------------------------------------------
    # snow new <ticket> <script> -> run snow-new-branch, which runs git new-branch in both repos
    # ----------------------------------------------------------------------
    if argv.first == "new"
      run_and_exit! "snow-new-branch #{argv[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow link <pr> <finance_pr> [package_pr] -> Create Slackable links for Zack
    # ----------------------------------------------------------------------
    if argv.first == "link"
      run_and_exit! "snow-link #{argv[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow backlog - show backlog file
    # ----------------------------------------------------------------------
    if argv.first == "backlog"
      run_and_exit! "snow-backlog #{argv[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow [check] [-e]
    # ----------------------------------------------------------------------

    if argv.first == "check"
      args = argv.dup
      args -= ['-e','--essential']

      if args.size > 1
        set_script args.last
      end
    end

    # ----------------------------------------------------------------------
    # snow-slam - git cram both repos and emit create pr links
    # ----------------------------------------------------------------------
    if argv.first == "slam"
      run_and_exit! "snow-slam #{argv[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow-auto - automatically fix what can be automatically fixed
    # ----------------------------------------------------------------------
    if argv.first == "auto"
      run_and_exit! "snow-autofix-all-the-things"
    end

    # ----------------------------------------------------------------------
    # Start the actual checking process
    # ----------------------------------------------------------------------

    puts "Checking #{get_script}..."

    # load file
    lines = IO.readlines(get_script).map(&:chomp)

    # ----------------------------------------------------------------------
    # Check Whitespace
    # ----------------------------------------------------------------------
    detect_unmanaged_whitespace lines if run_whitespace_detectors?

    # ----------------------------------------------------------------------
    # Check for PEP8 Conformance
    # ----------------------------------------------------------------------
    if !essential? && !quick?
      if skip_pep_check?
        puts section_title("SKIPPING PEP8 CONFORMANCE")
      else
        puts section_title("PEP8 CONFORMANCE")
        system "pep-check #{get_script}"
      end


      # puts section_title("BOILERPLATE DOCUMENTATION")
      # system "snow-check-boilerplate #{get_script}"
    end

    # ----------------------------------------------------------------------
    # Pre-check the file -- this has no fix, it's just a warning to be aware
    # that whenever we run the job we must include --skip_upload
    # ----------------------------------------------------------------------
    # check for --skip_upload
    # 2022-11-21 - removing; snow run picks this up automatically
    # detect_and_print_lines lines, "skip_upload", /--skip_upload/, essential: true

    # ======================================================================
    # BEGIN SQL CHANGES - Stuff that runs in Redshift but not in Snowflake
    # ======================================================================

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: DISTKEY, SORTKEY
    # ----------------------------------------------------------------------
    # DISTKEY, SORTKEY: just remove, it's fine
    detect_and_print_lines lines, "DISTKEY / SORTKEYS", /\b(DIS|SOR)TKEY\b/i, essential: true, autofix: "sed -E -i '/^.\\s*\\b(DIS|SOR)TKEY\\b .*$/d' #{get_script}"
    # if no conflicts found, this can be autofixed with sed -E -i '/\b(DIS|SOR)TKEY\b/d

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: * INTERVAL
    # ----------------------------------------------------------------------
    #
    # Don't use postgresq2l date interval arithmetic.
    #
    # Instead of
    # WHERE timestamp::DATE >= CURRENT_DATE - INTERVAL '2 days'
    #
    # We have to use DATEADD or DATEDIFF:
    #
    # WHERE timestamp >= DATEADD(day, -2, CURRENT_DATE)
    # WHERE DATEDIFF(day, timestamp, CURRENT_DATE) <= 2
    #
    # Or
    #
    # ==> LQQK LQQK LQQK ==> Watch out: This is an ok conversion:
    # contact_start - INTERVAL '6 hour'
    #
    # Watch for Spencer's `timestamp 'epoch' + seconds * interval '1 second'`,
    # it specifally converts easilly to just `TO_TIMESTAMP(seconds)`
    detect_and_print_lines lines, "Interval Arithmetic",
                           /\*\s*INTERVAL\b/i,
                           essential: true,
                           suggest: :replace_epoch_timestamp_with_interval_1_second

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: Postgres INT2, INT4, INT8
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Postgresql integer extension types", /\bINT(2|4|8)\b/i, essential: true

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: GEOGRAPHY
    # ----------------------------------------------------------------------
    # GEOGRAPHY data type: change to GEOMETRY data type
    detect_and_print_lines lines, "GEOGRAPHY", /\bGEOGRAPHY\b/i, essential: true

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: AT TIME ZONE
    # ----------------------------------------------------------------------
    # AT TIME ZONE '<zone>': change to CONVERT_TIMEZONE('<to_zone>', source_timestamp)
    #
    # Can this be automated? NO, this is some serious bullshit.
    #
    # DANGER DANGER DANGER:  Investigate. My snowflake sessions in DataGrip are
    # set to MST timezone. I fixed this (for how long? Guessing for the length
    # of "the session") with ALTER SESSION SET TIMEZONE = 'UTC'.
    #
    # let's say time = TO_TIMESTAMP('2022-09-13 09:00:00 +00:00'). That works in
    # both databases.
    #
    # Redshift's TO_TIMESTAMP will return a timestamp WITH TIMEZONE, so later on
    # doing time AT TIME ZONE 'MST' convert TO MST, subtract 7 hours, and drop
    # the TZ part. BUT!  Snowflake's TO_TIMESTAMP will return a timestamp
    # WITHOUT a timezone, which means doing CONVERT_TIMEZONE('MST', time) is
    # telling the database "hey!  This timestamp is already in MST, please tack
    # a timezone on it and convert it to UTC". That means Snowflake will ADD
    # seven hours, not subtract. UGH.

    # Okay. Okay. Okay. I found a thing that works.
    # First, we need to send "ALTER SESSION SET TIMEZONE = 'UTC' as often as we
    # need to. I'm guessing every script should start off that way.

    # TEST THIS: It might be DataGrip that was messing that up by "helpfully"
    # reading my laptop's local settings. So when we run a script, it might
    # already be in UTC. Nope. Maybe? The server appears to be in the US/Los
    # Angeles time zone, which is PT with DST. UGH.

    # Some cases, maybe. E.g. field ATZ 'MST' ATZ 'EST'
    # would convert to CONVERT_TIMEZONE('EST', 'MST', field)
    if run_timezone_detectors?
      detect_and_print_lines lines, "AT TIME ZONE", /\bAT TIME ZONE\b/i, essential: true, suggest: :replace_at_time_zone
    end

    # ----------------------------------------------------------------------
    # TODO: CREATE TABLE ( LIKE )
    # ----------------------------------------------------------------------
    # change CREATE TABLE foo ( LIKE bar ) -> CREATE TABLE foo LIKE bar
    #
    # Needs multiline search to find "create table" on one line and "( like" on
    # the next
    #
    # Originally found in etl_f_delighted_survey_responses.py
    # ----------------------------------------------------------------------
    # detect_and_print_lines lines, "name?", finder: :find_create_table_foo_paren_like_bar

    # ----------------------------------------------------------------------
    # UNLOAD
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "UNLOAD", /\bUNLOAD\b\s*\(/i, essential: true

    # ----------------------------------------------------------------------
    # TRUNC
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "TRUNC (Check by hand, must have 2 args)", /\bTRUNC\b\s*\(/i, warning: true

    # ----------------------------------------------------------------------
    # SIMILAR TO -> RLIKE
    #
    # RED: <column> SIMILAR TO <regex>
    # SNOW: RLIKE(<column>, <regex>)
    #
    # NOTE: For simple expressions favor LIKE ANY ('%bleh%', '%glorp%')
    # Use RLIKE when the regexp is genuinely complicated
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "SIMILAR TO", /(similar to|~)/i, suggest: :replace_similar_to_with_like_any, unmatch: /os\.path\.expanduser/

    if run_json_detectors?
      # ----------------------------------------------------------------------
      # JSON_EXTRACT_* functions
      #
      # 2022-11-21: Per Zack, consider replacing with PARSE_JSON and handling the
      # resulting variant instead. DS-1746 came out much cleaner that way:
      #
      # Strict Replacement: JSON_EXTRACT_PATH_TEXT(json, '[' || level || ']')
      # Cleaner Replacement: PARSE_JSON(json)[level]
      #
      # JSON_EXTRACT_PATH_TEXT - Path may need tweaking but should port fine as
      # long as it only has 2 or 3 arguments and if the third argument is present,
      # it is TRUE. (Redshift allows TRUE/FALSE, Snowflake only does the TRUE
      # version. Our codebase only does TRUE to the best of my knowledge).
      #
      # JSON_EXTRACT_ARRAY_ELEMENT_TEXT - This can be expressed as a
      # JSON_EXTRACT_PATH_TEXT call:
      #
      # JEAET('json_array', 3) -> JEPT('json_array', '[3]')
      detect_and_print_lines lines, "JSON MANIPULATION FUNCTIONS",
                             %r{JSON_EXTRACT_PATH_TEXT|JSON_EXTRACT_ARRAY_ELEMENT_TEXT|IS_VALID_JSON|IS_VALID_JSON_ARRAY|JSON_ARRAY_LENGTH|JSON_EXTRACT_ARRAY_ELEMENT_TEXT|JSON_PARSE|CAN_JSON_PARSE|JSON_SERIALIZE|JSON_SERIALIZE_TO_VARBYTE},
                             essential: true
      # , unmatch: /JSON_EXTRACT_PATH_TEXT\([^,]+,[^,]+\)/ # JEPT/2 good; JEPT/3 bad.
    end

    if run_quotes_detectors?
      detect_and_print_lines lines, "Doubled Quotes (Check By Hand)", /''.*''/, warning: true

      detect_and_print_lines lines, ".replace(\"'\", \"''\") quotes with doubled quotes", /\.replace[\(\)\,\s\'\"]+$/, warning: true
    end


    if run_date_detectors?
      # ----------------------------------------------------------------------
      # DATE_ADD('unit',num, column) -> DATEADD(unit, num, column)
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "DATE_ADD", /DATE_ADD\('\w+'/i, autofix: %Q|sed -i -E "s/DATE_ADD\\('([^']+)'/DATEADD(\\1/ig" #{get_script}|

      # ----------------------------------------------------------------------
      # DATE_DIFF('unit',num, column) -> DATEDIFF(unit, num, column)
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "DATE_DIFF", /DATE_DIFF\('\w+'/i, autofix: %Q|sed -i -E "s/DATE_DIFF\\('([^']+)'/DATEDIFF(\\1/ig" #{get_script}|

      # ----------------------------------------------------------------------
      # DATEPART -> DATE_PART
      #
      # YES, this is the OPPOSITE of date add and diff: Redshift does NOT have an
      # underscore and snowflake requires one. At least the units are the same;
      # Redshift actually uses bareword identifiers here. :shrug:
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "DATEPART", /\bDATEPART\b/i, autofix: %Q|sed -i -E "s/\\bDATEPART\\b/DATE_PART/ig" #{get_script}|
    end

    # ----------------------------------------------------------------------
    # BTRIM
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "BTRIM", /\bBTRIM\(/i, autofix: "sed -i -E 's/BTRIM\\(/TRIM\\(/g' #{get_script}"

    # ----------------------------------------------------------------------
    # TO_CHAR
    #
    # You can specify a fixed char size for a numeric field with 9's. Redshift
    # allows this to be an integer. Snowflake demands a string:
    #
    # Red:
    # SELECT '[' || TO_CHAR(99, 999) || ']'       # [ 99]
    # SELECT '[' || TO_CHAR(9999, 999) || ']'     # [###]
    #
    # Sno:
    # SELECT '[' || TO_CHAR(99, '999') || ']'     # [ 99]
    # SELECT '[' || TO_CHAR(9999, '999') || ']'   # [###]
    #
    # match will unmatch if there is a last argument in quotes that's like
    # 999,999D99 or YYYY-MM-DD etc.
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "TO_CHAR (with numeric format)", /to_char/i, unmatch: /'[YMD\-9,]+'/

    # ----------------------------------------------------------------------
    # WITH NO SCHEMA BINDING -> just delete the line
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "WITH NO SCHEMA BINDING", /WITH NO SCHEMA BINDING/i

    # ----------------------------------------------------------------------
    # BOOL -> BOOLEAN
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "BOOL", /\bBOOL\b/i, autofix: "(!!!CAUTION: UNTESTED!!!) sed -i -E 's/\bBOOL\b/BOOLEAN/ig' #{get_script}"

    # ----------------------------------------------------------------------
    # [WITH] days AS
    #
    # You can have a CTE named days in redshift, but in snowflake it's a
    # keyword.
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "DAYS AS", /\bdays\s+as\b/i, essential: true

    # ======================================================================
    # END SQL CHANGES
    # ======================================================================

    # ----------------------------------------------------------------------
    # NPM-style Lists
    # ----------------------------------------------------------------------
    # I deeply dislike NPM-style leading-commas. Casey loves them. Be kind to
    # the maintainer. If there's a motherhuge query that only Casey can debug,
    # or the entire file is consistently in NPM-style, leave it in his preferred
    # style. If there's a mix of styles or it's something I have to maintain,
    # change it. Unless there's already a good reason to be editing a query,
    # don't change its style.
    detect_and_print_lines lines, "NPM-style Lists (Leading Commas)", /^\s*,\s*\S+/, disabled: true

    # ----------------------------------------------------------------------
    # Redshift S3 buckets
    # These will probably stay in the project until/unless we can find out who
    # is reading from these s3 buckets.
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Redshift S3 Buckets", %r|s3.*redshift|i, essential: true

    # ----------------------------------------------------------------------
    # Redshift mentioned anywhere
    # ----------------------------------------------------------------------
    # (<!s3) is "negative lookbehind". It means find redshift, but not if it was
    # preceded by s3 (which is found by Redshift S3 buckets above)
    detect_and_print_lines lines, "Redshift mentioned anywhere", /redshift/i, unmatch: %r|s3.*redshift|

    # ----------------------------------------------------------------------
    # Secretly blocked by DS-1704: d_team_list_rac_cs_gsheet
    # ----------------------------------------------------------------------
    # detect_and_print_lines lines, "Secretly blocked by DS-1704", %r|D_TEAM_LIST_RAC_CS_GSHEET|i, essential: true

    # ----------------------------------------------------------------------
    # Update Python String Formatting
    # ----------------------------------------------------------------------
    if run_python2_detectors?
      detect_and_print_lines lines, "Python 2 String Formatting", %r|(?<!%)%[\d\.]*[sdf].*|, unmatch: %r{(str[pf]time|%Y-%m-%d|LIKE ANY)}
    end

    # ----------------------------------------------------------------------
    # os.chdir to self: remove
    # ----------------------------------------------------------------------
    # If we see this exact line, just remove it:
    # os.chdir(os.path.dirname(os.path.abspath(__file__)))
    #
    # If we see another os.chdir, flag it as a warning.
    # the autofix here is sed -E -i 's/\bos.chdir\b/d'
    detect_and_print_lines lines, "Potentially Useless os.chdir", /\bos.chdir\b/, autofix: "sed -E -i '/\\bos.chdir\\b/d' #{get_script}"

    # ----------------------------------------------------------------------
    # Excessive globals in main
    # ----------------------------------------------------------------------
    # Needs custom finder. Count up lines after def main that contain "global"

    # ----------------------------------------------------------------------
    # Mutual globals
    # ----------------------------------------------------------------------
    # Look for globals in other methods. Don't overautoanalyze the code (not
    # worth the time to write it) but maybe a note of "here we have global
    # start_date, see where it is used and maybe upgrade it to a parameter
    # instead of a global".

    if run_exception_detectors?
      # ----------------------------------------------------------------------
      # except Exception
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "Global Exception Handling", /\bexcept Exception\b/, warning: true
    end

    # ----------------------------------------------------------------------
    # job_config must not be optional
    # ----------------------------------------------------------------------
    # this is a sweeping grep, should work 95% of the time. SOME false positives
    # will occur that COULD be avoided by checking etl_nightly for the
    # run_method and grepping only for THAT method but that will probably take
    # more time than just weeding the falses out by hand. Looks like there are
    # only 2 that take config=None while all the other true positives are
    # job_config=None, so.
    detect_and_print_lines lines, "Optional job_config", /def (run|main)\(.*config=/

    # ----------------------------------------------------------------------
    # testing for job_config == None or job_config is None
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Testing for presence of job_config", /\bjob_config\s+(is|==)\s+None\b/

    # ----------------------------------------------------------------------
    # except Exception:
    # ----------------------------------------------------------------------
    # detect_and_print_lines lines, "except Exception:", /except Exception:/, autofix: "sed -E -i 's/except Exception:/except:/' #{get_script}"

    # ----------------------------------------------------------------------
    # unsize varchars
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Unsize VARCHAR", /varchar\(\d+\)/i, autofix: %Q|ruby -i -n -e 'puts $_.gsub(/varchar\\(\\d+\\)/, "varchar").gsub(/VARCHAR\\(\\d+\\)/, "VARCHAR")' #{get_script}|

    # ----------------------------------------------------------------------
    # TODO: no-op reraise in exception
    # marketing_device_traffic.py
    #
    # <blank line(s)>
    # except.*:
    #     raise
    #
    # Fix: delete those lines
    # ----------------------------------------------------------------------

    # ----------------------------------------------------------------------
    # case and paste help messages for arparse
    # search for different options but same help messages, e.g.
    #     parser.add_argument('--files', help='Start Date', required=True)
    #     parser.add_argument('--start_date', help='Start Date', required=False)
    #     parser.add_argument('--end_date', help='Start Date', required=False)
    # ----------------------------------------------------------------------


    # ----------------------------------------------------------------------
    # TODO: etl_client.close() before finally
    # Actually, there's a whole common thing here:
    #
    # 1. Detection:
    #   1. Find the 'except Exception[ as e]:' line
    #   2. Is it followed only by 1 or more of etl_client.close(), dbo.closue, or raise
    #   3. It MAY be preceded by 1 or 2 blank lines
    #   4. Those blank lines may by preceded by another etl_client.close() (and
    #   potentially dbo.close(), but that's super rare, so ignore and faster to
    #   fix it by hand when it occurs)
    #
    # 2. Correction:
    #   1. Delete the preceding etl close
    #   2. Insert 1 blank line between that and the finally
    #   3. Change the except Exception line to finally, keeping same indent
    #   4. Delete the raise
    # ----------------------------------------------------------------------

    # ----------------------------------------------------------------------
    # Hidden blockers (depends on an unmigrated table)
    # ----------------------------------------------------------------------
    detect_potentially_unmigrated_tables lines, essential: true

    # ----------------------------------------------------------------------
    # main() bootstrapper
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "main() bootstrapper", /if\s*__name__\s*==\s*["']__main__["']\s*:/

    # ----------------------------------------------------------------------
    # WARN if job is mcloud or sf but does not have --skip_upload
    # Or if it's gsheet or API and does not have --test
    # ----------------------------------------------------------------------
    if run_missing_detectors?
      detect_missing_skip_upload
      detect_missing_test_mode
      detect_missing_recipients_override
    end

    # ----------------------------------------------------------------------
    # CREATE TABLE a (LIKE b) -> CREATE TABLE a LIKE b
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "CREATE TABLE LIKE [1/3]", /\bCREATE\s+TABLE\b.*\(LIKE/i, suggest: :replace_inline_create_table_like, essential: true
    detect_and_print_multilines lines, "CREATE TABLE LIKE [2/3]", [/\bCREATE\s+TABLE\b.*\(/i, /LIKE/i], essential: true
    detect_and_print_multilines lines, "CREATE TABLE LIKE [3/3]", [/\bCREATE\s+TABLE\b/i, /\(\s*LIKE/i], essential: true

    # ----------------------------------------------------------------------
    # ENCODE
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "ENCODE", /\bencode\s+\S+/i, essential: true

    # ----------------------------------------------------------------------
    # GREATEST/LEAST -> COALESCE(GREATEST/LEAST(...),value)
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "GREATEST/LEAST", /\b(greatest|least)\b/i, essential: true # , unmatch: /coalesce\((greatest|least)\b\(/i

    # ----------------------------------------------------------------------
    # COALESCE/1
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "COALESCE/1", /coalesce\([^\(,]+\)/i, essential: true

    if run_whitespace_detectors?
      # ----------------------------------------------------------------------
      # TABS
      # ----------------------------------------------------------------------
      detect_and_print_lines lines, "TABS", /\t/
    end
  end
end


if __FILE__ == $0
  SnowflakeManglerApp.new.run
end
