#!/usr/bin/env ruby
# snow - Command suite to support moving to snowflake

require 'colorize'
require 'optimist'

class SnowflakeManglerApp
  def checklist
    run_and_exit! "snow-show-checklist"
  end

  def set_script(script)
    puts "set_script(#{script})"
    @get_script = script
  end

  def get_script
    return @get_script if @get_script

    @get_script = `get-ds-script`.strip
    @get_script = ENV['SCRIPT'] if @get_script.empty?
    @get_script
  end

  # Some commands do not need the script var to be set; in fact they need to be
  # able to when it is not set (because they cause it to become set)
  def need_script?
    !["go", "new"].include?(ARGV.first)
  end

  # Was this script invoked with --essential?
  def essential?
    ARGV.include?('--essential') || ARGV.include?('-e')
  end

  # Was this script invoked with --list?
  def list?
    ARGV.include?('--list') || ARGV.include?('-l') || ARGV.include?('list')
  end

  def help?
    ARGV.include?('--help') || ARGV.include?('-h') || ARGV.include?('help')
  end

  def section_title(title)
    "====%s" % " #{title} ".ljust(80, "=").cyan
  end

  def log_ok
    puts " OK ".bold.light_white.on_green
  end

  # This is turning into a bit of a swiss army kitchen sink...
  # lines:     array of lines to check
  # title:     section title to print if matches are found
  # regex:     use this to detect matching lines. If regex is an array of regexes,
  #            finds matches on adjacent lines
  # disabled:  this check is turned off. Do not run, even if marked essential
  # essential: this check is essential. If we're running snow -e, skip
  #            inessential checks
  # autofix:   text containing an autofix. Usually a bash command applied to the
  #            whole file
  # suggest:   a method that receives the matching line, and returns a suggested
  #            fix.
  # unmatch:   ignore lines that match regex but ALSO match this regex.
  def detect_and_print_lines(
        lines,
        title,
        regex,
        essential: false,
        disabled: false,
        autofix: nil,
        suggest: nil,
        unmatch: nil
      )
    return if disabled
    return if essential? && !essential
    puts section_title title
    # OOF.
    # match the regex AND (we have no unmatch, or line is not unmatched)
    if lines.any? { |line| line =~ regex && (!unmatch || line !~ unmatch) }
      puts "Autofix with: #{autofix}" if autofix
      # Yeah, turn this into lines_with_indexes = finder_method, we can
      # write a generic version to do what this regex does, and then we can
      # pass in custom finder methods for multiline and trickier stuff
      lines.each.with_index do |line, index|
        if line =~ regex && (unmatch.nil? || line !~ unmatch)
          puts "%4d: %s" % [index+1, line]
        end
        if suggest && suggestion = send(suggest, line)
          puts "Replacement: #{suggestion}"
        end
      end
    else
      log_ok
    end
  end

  def detect_and_print_multilines(
        lines,
        title,
        regexes,
        essential: false,
        disabled: false,
        autofix: nil,
        suggest: nil,
        unmatch: nil
      )
    return if disabled
    return if essential? && !essential
    puts section_title title

    # given n regexes, at line[m], then...
    # Simple Match For Now:
    # 1. lines[m..m+n].zip(regexes).all? {|line, regex| line =~ regex }
    #
    # More complex matching might allows for same-line stuff, e.g. if we are
    # matching on [/create/, /table/, /like/], we should match if:
    #
    # (line[m] =~ /create.*table.*like/) ||
    # (line[n] =~ /create/ && line[n+1] =~ /table.*like/) ||
    # (line[n] =~ /create.*table/ && line[n+1] =~ /like/) ||
    # (line[n] =~ /create/ && line[n+1] =~ /table/ && line[n+2] =~ /like/)
    no_matches = true
    first = true
    0.upto(lines.size - regexes.size) do |main_offset|
      if (0...regexes.size).map {|j| [lines[main_offset+j], regexes[j]] }.all? {|line, regex| line =~ regex }
        no_matches = false
        if first
          first = false
        else
          puts "--"
        end
        puts (0...regexes.size).map {|j| "#{main_offset+j}: #{lines[main_offset+j]}" }
      end
    end

    log_ok if no_matches
  end

  def detect_missing_skip_upload
    puts section_title "Skip Upload"
    if should_be_skippable? && !skippable?
      puts "File should have skip_upload but does not"
    else
      log_ok
    end
  end

  def usage
    puts "I COULD be bothered to parse ARGV, but I don't want YOU to be bothered typing all the time. Sorry not sorry, you're welcome."
    puts "Try doing this, and maybe put it in .bashrc while you're at it:"
    puts "export SCRIPT=#{ARGV.join(' ')}"
    puts "^^^ DEPRECATED. Use set-ds-script instead!"
  end

  def skippable?
    IO.readlines(get_script).any? { |line| line =~ /--skip_upload/ }
  end

  def should_be_skippable?
    get_script.include?("mcloud") || get_script.include?("_sf_")
  end

  def run_and_exit!(command)
    system command
    exit $?.exitstatus
  end

  def usage
    str = <<~USAGE
snow - master control script for snowflake job migration

snow [<command> [<options>]]

Most commands use get-ds-script, and/or check the last line of ~/.ds_script for
the name of the script filename. For legacy reasons this is called the SCRIPT
file. It should point to the job file relative from the warehouse
directory. E.g. for ~/dataservices/etl/warehouse/rac/rac_job1.py, do
set-ds-script rac/rac_job1.py

Many jobs check that the current branch matches the SCRIPT file. This puts an
extra bit of bookkeeping on you, but it's actually there to ensure that you're
not accidentally merging to master or snowflake-moveover. Again.

Commands/options:
help, -h, --help                 Show this message
list, -l, --list                 Print out the migration checklist
check [-e|--essential] [job.py]  Check the [current] job for [only essential] coding violations
job [job]                        Show current [or specified] job's configuration
job s[nowflake]                  Convert job configuration to snowflake
job r[edshift]                   Convert job configuration back to redshift
run                              Run the current job
main                             Checkout the snowflake main branch in both repos
master                           Checkout master in both repos
new <ticket> <job.py>            Create a new feature branch in both repos
link <pr> <finance_pr> [pkg_pr]  Create links template for Zack

If no commands are given, snow will run check.
    USAGE
  end

  def replace_at_time_zone(line)
    # AND t.contact_start AT TIME ZONE 'UTC' AT TIME ZONE 'America/Denver' >= '2021-01-23'
    # ,MIN(v.created_at) AT TIME ZONE 'UTC' AT TIME ZONE 'America/Denver' AS awaiting_delivery_at_local
    regex = /([()_\w\.]+) AT TIME ZONE '([^']+)' AT TIME ZONE '([^']+)'/
    return unless line =~ regex

    line.gsub(regex, "CONVERT_TIMEZONE('#{$2}', '#{$3}', #{$1})")
  end

  # This doesn't get all the cases, but it does get the ones that are inline.
  def replace_inline_create_table_like(line)
    regex = /\(LIKE (.+?)\)/
    return unless line =~ regex

    line.gsub(regex, "LIKE #{$1}")
  end

  def replace_epoch_timestamp_with_interal_1_second(line)
    regex = /timestamp\s+'epoch'\s*\+\s*(.*?)\*\s*interval\s+'1 second'/i

    return unless line =~ regex

    replacement = $1.strip
    replacement = replacement[1..] if replacement.start_with? "("
    replacement = replacement[..-2] if replacement.end_with? ")"
    replacement = replacement.strip

    replacement = "TO_TIMESTAMP(#{replacement})"

    line.gsub(regex, replacement)
  end

  # THE MAIN APP METHOD, LET'S GO
  def run
    # ----------------------------------------------------------------------
    # 1. Check for --help and --list
    # ----------------------------------------------------------------------
    if help?
      puts usage
      exit 0
    end

    # if -l or --list, show checklist and exit
    if list?
      checklist
      # puts "HIT ENTER TO EXIT"
      # $stdin.gets
      exit 0
    end

    # Override script if job is preview
    if ARGV.first == "preview"
      set_script ARGV[1]
    end

    # 2.2 If you do not have SCRIPT set, I tell you about SCRIPT and exit
    if need_script? && (get_script.nil? || get_script.empty?)
      puts "You must run set-ds-script <path/to/job.py> first."
      usage
      exit -1
    end

    # ----------------------------------------------------------------------
    # 3. dispatch/handle commands
    # ----------------------------------------------------------------------

    # ----------------------------------------------------------------------
    # snow run -> dsetl_nightly
    # dsetl_nightly reads SCRIPT from rc file; Use dsetl to specify from CLI
    # ----------------------------------------------------------------------
    if ARGV.first == "run"
      command = "dsetl_nightly"
      command += " --skip_upload" if skippable?
      command += " " + ARGV[1..].join(" ")
      run_and_exit! command
    end

    # ----------------------------------------------------------------------
    # snow main -> snow-go-main
    # ----------------------------------------------------------------------
    if ARGV.first == "main"
      run_and_exit! "snow-go-main"
    end

    # ----------------------------------------------------------------------
    # snow master -> snow-go-master
    # ----------------------------------------------------------------------
    if ARGV.first == "master"
      run_and_exit! "snow-go-master"
    end

    # ----------------------------------------------------------------------
    # snow go <id> -> snow-go-branch
    # ----------------------------------------------------------------------
    if ARGV.first == "go"
      run_and_exit! "snow-go-branch #{ARGV[1..] * ' '}"
    end

    # ----------------------------------------------------------------------
    # snow rebase [parent_branch] -> snow-rebase
    # ----------------------------------------------------------------------
    if ARGV.first == "rebase"
      run_and_exit! "snow-rebase #{ARGV[1..] * ' '}"
    end

    # ----------------------------------------------------------------------
    # snow job -> snow-show-job-config
    # ----------------------------------------------------------------------
    if ARGV.first == "job"
      if ARGV.size == 1
        run_and_exit! "snow-job-show-config #{ARGV[1..].join(' ')}"
      elsif ARGV[1] == "defaults" || ARGV[1] == "d"
        run_and_exit! "snow-job-show-config --defaults"
      elsif ARGV[1] == "redshift" || ARGV[1] == "r"
        run_and_exit! "snow-job-convert-to-redshift #{get_script}"
      elsif ARGV[1] == "snowflake" || ARGV[1] == "s"
        run_and_exit! "snow-job-convert-to-snowflake #{get_script}"
      end
    end

    # ----------------------------------------------------------------------
    # snow new <ticket> <script> -> run snow-new-branch, which runs git new-branch in both repos
    # ----------------------------------------------------------------------
    if ARGV.first == "new"
      run_and_exit! "snow-new-branch #{ARGV[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow link <pr> <finance_pr> [package_pr] -> Create Slackable links for Zack
    # ----------------------------------------------------------------------
    if ARGV.first == "link"
      run_and_exit! "snow-make-links #{ARGV[1..].join(' ')}"
    end

    # ----------------------------------------------------------------------
    # snow [check] [-e]
    #
    # TODO: snow -q, --quick just emits a single OK or "5 problems found on 23
    # lines" etc. Have to rework the detectors to return the number of problems
    # found.
    #
    # TODO: snow -w, --work emits only the detectors that fail, so you can see
    # remaining work
    #
    # TODO: port this to Optimist. The arg processing is killing me here
    # ----------------------------------------------------------------------

    if ARGV.first == "check"
      args = ARGV.dup
      args -= ['-e','--essential']

      if args.size > 1
        set_script args.last
      end
    end

    # ----------------------------------------------------------------------
    # snow-slam - git cram both repos and emit create pr links
    # ----------------------------------------------------------------------
    if ARGV.first == "slam"
      run_and_exit! "snow-slam"
    end

    # ----------------------------------------------------------------------
    # snow-auto - automatically fix what can be automatically fixed
    # ----------------------------------------------------------------------
    if ARGV.first == "auto"
      run_and_exit! "snow-autofix-all-the-things"
    end

    # ----------------------------------------------------------------------
    # Start the actual checking process
    # ----------------------------------------------------------------------

    puts "Checking #{get_script}..."

    # load file
    lines = IO.readlines(get_script).map(&:rstrip)

    # ----------------------------------------------------------------------
    # Check for PEP8 Conformance
    # ----------------------------------------------------------------------
    if !essential?
      puts section_title("PEP8 CONFORMANCE")
      system "pep-check #{get_script}"
    end

    # ----------------------------------------------------------------------
    # Pre-check the file -- this has no fix, it's just a warning to be aware
    # that whenever we run the job we must include --skip_upload
    # ----------------------------------------------------------------------
    # check for --skip_upload
    detect_and_print_lines lines, "skip_upload", /--skip_upload/, essential: true

    # ======================================================================
    # BEGIN SQL CHANGES - Stuff that runs in Redshift but not in Snowflake
    # ======================================================================

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: DISTKEY, SORTKEY
    # ----------------------------------------------------------------------
    # DISTKEY, SORTKEY: just remove, it's fine
    detect_and_print_lines lines, "DISTKEY / SORTKEYS", /\b(DIS|SOR)TKEY\b/i, essential: true, autofix: "sed -E -i '/\\b(DIS|SOR)TKEY\\b/d' #{get_script}"
    # if no conflicts found, this can be autofixed with sed -E -i '/\b(DIS|SOR)TKEY\b/d

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax:  INTERVAL
    # ----------------------------------------------------------------------
    #
    # Don't use postgresq2l date interval arithmetic.
    #
    # Instead of
    # WHERE timestamp::DATE >= CURRENT_DATE - INTERVAL '2 days'
    #
    # We have to use DATEADD or DATEDIFF:
    #
    # WHERE timestamp >= DATEADD(day, -2, CURRENT_DATE)
    # WHERE DATEDIFF(day, timestamp, CURRENT_DATE) <= 2
    #
    # Or
    #
    # But watch out: This is an ok conversion:
    # contact_start - interval '6 hour'
    # DATEADD(contact_start, -6, 'hour')
    #
    # Watch for Spencer's `timestamp 'epoch' + seconds * interval '1 second'`,
    # it specifally converts easilly to just `TO_TIMESTAMP(seconds)`
    detect_and_print_lines lines, "Interval Arithmetic", /\bINTERVAL\b/i, essential: true, suggest: :replace_epoch_timestamp_with_interal_1_second

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: Postgres INT2, INT4, INT8
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Postgresql integer extension types", /\bINT(2|4|8)\b/i, essential: true

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: GEOMETRY, ST_GEOMFROMTEXT
    # ----------------------------------------------------------------------
    # GEOMETRY data type: change to GEOGRAPHY data type
    #
    # 2021-11-07: Cancel this change; We have updated Snowflake to calculate
    # GEOGRAPHY correctly. In fact, if we run snow on an older job, we want it
    # to flag the GEOMETRY as an error. So don't just disable this job; we need
    # to reverse it.
    #
    # https://docs.snowflake.com/en/sql-reference/data-types-geospatial.html
    # detect_and_print_lines lines, "GEOMETRY", /\bGEOMETRY\b/i, essential: true
    detect_and_print_lines lines, "GEOGRAPHY", /\bGEOGRAPHY\b/i, essential: true

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: ST_GEOMFROMTEXT
    # ----------------------------------------------------------------------
    # ST_GEOMFROMTEXT function: change to TO_GEOGRAPHY
    # https://docs.snowflake.com/en/sql-reference/functions/to_geography.html
    detect_and_print_lines lines, "ST_GEOMFROMTEXT (***TEST THIS***, GEOMETRY IS CANCELED)", /\bST_GEOMFROMTEXT\b/i, essential: true

    # ----------------------------------------------------------------------
    # Illegal Snowflake Syntax: AT TIME ZONE
    # ----------------------------------------------------------------------
    # AT TIME ZONE '<zone>': change to CONVERT_TIMEZONE('<to_zone>', source_timestamp)
    #
    # Can this be automated? NO, this is some serious bullshit.
    #
    # DANGER DANGER DANGER:  Investigate. My snowflake sessions in DataGrip are
    # set to MST timezone. I fixed this (for how long? Guessing for the length
    # of "the session") with ALTER SESSION SET TIMEZONE = 'UTC'.
    #
    # let's say time = TO_TIMESTAMP('2022-09-13 09:00:00 +00:00'). That works in
    # both databases.
    #
    # Redshift's TO_TIMESTAMP will return a timestamp WITH TIMEZONE, so later on
    # doing time AT TIME ZONE 'MST' convert TO MST, subtract 7 hours, and drop
    # the TZ part. BUT!  Snowflake's TO_TIMESTAMP will return a timestamp
    # WITHOUT a timezone, which means doing CONVERT_TIMEZONE('MST', time) is
    # telling the database "hey!  This timestamp is already in MST, please tack
    # a timezone on it and convert it to UTC". That means Snowflake will ADD
    # seven hours, not subtract. UGH.

    # Okay. Okay. Okay. I found a thing that works.
    # First, we need to send "ALTER SESSION SET TIMEZONE = 'UTC' as often as we
    # need to. I'm guessing every script should start off that way.

    # TEST THIS: It might be DataGrip that was messing that up by "helpfully"
    # reading my laptop's local settings. So when we run a script, it might
    # already be in UTC. Nope. Maybe? The server appears to be in the US/Los
    # Angeles time zone, which is PT with DST. UGH.

    # Some cases, maybe. E.g. field ATZ 'MST' ATZ 'EST'
    # would convert to CONVERT_TIMEZONE('EST', 'MST', field)
    detect_and_print_lines lines, "AT TIME ZONE", /\bAT TIME ZONE\b/i, essential: true, suggest: :replace_at_time_zone

    # ----------------------------------------------------------------------
    # TODO: CREATE TABLE ( LIKE )
    # ----------------------------------------------------------------------
    # change CREATE TABLE foo ( LIKE bar ) -> CREATE TABLE foo LIKE bar
    #
    # Needs multiline search to find "create table" on one line and "( like" on
    # the next
    #
    # Originally found in etl_f_delighted_survey_responses.py
    # ----------------------------------------------------------------------
    # detect_and_print_lines lines, "name?", finder: :find_create_table_foo_paren_like_bar

    # ======================================================================
    # END SQL CHANGES
    # ======================================================================

    # ----------------------------------------------------------------------
    # NPM-style Lists
    # ----------------------------------------------------------------------
    # I deeply dislike NPM-style leading-commas. Casey loves them. Be kind to
    # the maintainer. If there's a motherhuge query that only Casey can debug,
    # or the entire file is consistently in NPM-style, leave it in his preferred
    # style. If there's a mix of styles or it's something I have to maintain,
    # change it. Unless there's already a good reason to be editing a query,
    # don't change its style.
    detect_and_print_lines lines, "NPM-style Lists (Leading Commas)", /^\s*,\s*\S+/, disabled: true

    # ----------------------------------------------------------------------
    # Redshift S3 buckets
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Redshift S3 Buckets", %r|s3.*redshift|i, essential: true

    # ----------------------------------------------------------------------
    # Redshift mentioned anywhere
    # ----------------------------------------------------------------------
    # (<!s3) is "negative lookbehind". It means find redshift, but not if it was
    # preceded by s3 (which is found by Redshift S3 buckets above)
    detect_and_print_lines lines, "Redshift mentioned anywhere", %r|(<!s3).*redshift|i

    # ----------------------------------------------------------------------
    # Secretly blocked by DS-1704: d_team_list_rac_cs_gsheet
    # ----------------------------------------------------------------------
    # detect_and_print_lines lines, "Secretly blocked by DS-1704", %r|D_TEAM_LIST_RAC_CS_GSHEET|i, essential: true

    # ----------------------------------------------------------------------
    # Update Python String Formatting
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Python 2 String Formatting", %r|(?<!%)%[\d\.]*[sdf].*|, unmatch: %r{(str[pf]time|%Y-%m-%d)}

    # ----------------------------------------------------------------------
    # os.chdir to self: remove
    # ----------------------------------------------------------------------
    # If we see this exact line, just remove it:
    # os.chdir(os.path.dirname(os.path.abspath(__file__)))
    #
    # If we see another os.chdir, flag it as a warning.
    # the autofix here is sed -E -i 's/\bos.chdir\b/d'
    detect_and_print_lines lines, "Potentially Useless os.chdir", /\bos.chdir\b/, autofix: "sed -E -i '/\\bos.chdir\\b/d' #{get_script}"

    # ----------------------------------------------------------------------
    # Excessive globals in main
    # ----------------------------------------------------------------------
    # Needs custom finder. Count up lines after def main that contain "global"

    # ----------------------------------------------------------------------
    # Mutual globals
    # ----------------------------------------------------------------------
    # Look for globals in other methods. Don't overautoanalyze the code (not
    # worth the time to write it) but maybe a note of "here we have global
    # start_date, see where it is used and maybe upgrade it to a parameter
    # instead of a global".

    # ----------------------------------------------------------------------
    # except Exception
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Global Exception Handling", /\bexcept Exception\b/

    # egregious exception handling -- wait, what if instead of a regex we pass
    # a detector symbol?  and detector method receives ALL of lines. Then it can
    # check for e.g. this regex followed by "raise" and then blank line(s).
    #
    # Argh. No, it's too much of a partial solution (but maybe yes b/c yagni?),
    # what if we want to add line numbers to th eautofix? I.e. This could be
    # autofix: 'sed "{LINE},{LINE+1}d"' or similar.
    #
    # The alternate approach is to have a "found problem" object that has the
    # line where a problem occurs AND its location. Then we can feed that
    # through an autofix/suggest class. Maybe we keep the bare regex thing, but
    # allow passing in an object that has a detection method, autofix method,
    # suggest method, etc. (Basically IOC for the detect and fix pattern).

    # ----------------------------------------------------------------------
    # Outdented SQL
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Outdented SQL (Not Fully Implemented - Check By Hand)", /^(FROM|WHERE|JOIN) /, disabled: true

    # ----------------------------------------------------------------------
    # Long lines
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Egregiously Long Lines (Check by hand)", /^.{220,}$/, disabled: true

    # ----------------------------------------------------------------------
    # Overcomplicated format() calls
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Overly short format() calls (Check by hand)", /\.format\([^){,20}]\)/, disabled: true

    # ----------------------------------------------------------------------
    # job_config must not be optional
    # ----------------------------------------------------------------------
    # this is a sweeping grep, should work 95% of the time. SOME false positives
    # will occur that COULD be avoided by checking etl_nightly for the
    # run_method and grepping only for THAT method but that will probably take
    # more time than just weeding the falses out by hand. Looks like there are
    # only 2 that take config=None while all the other true positives are
    # job_config=None, so.
    detect_and_print_lines lines, "Optional job_config", /def (run|main)\(.*config=/

    # ----------------------------------------------------------------------
    # testing for job_config == None or job_config is None
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Testing for presence of job_config", /\bjob_config\s+(is|==)\s+None\b/

    # ----------------------------------------------------------------------
    # except Exception:
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "except Exception:", /except Exception:/, autofix: "sed -E -i 's/except Exception:/except:/' #{get_script}"

    # ----------------------------------------------------------------------
    # unsize varchars
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "Unsize VARCHAR", /varchar\(\d+\)/i, autofix: %Q|ruby -i -n -e 'puts $_.gsub(/varchar\\(\\d+\\)/, "varchar").gsub(/VARCHAR\\(\\d+\\)/, "VARCHAR")' #{get_script}|

    # ----------------------------------------------------------------------
    # similar to
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "SIMILAR TO", /similar to/i

    # ----------------------------------------------------------------------
    # TODO: no-op reraise in exception
    # marketing_device_traffic.py
    #
    # <blank line(s)>
    # except.*:
    #     raise
    #
    # Fix: delete those lines
    # ----------------------------------------------------------------------

    # ----------------------------------------------------------------------
    # case and paste help messages for arparse
    # search for different options but same help messages, e.g.
    #     parser.add_argument('--files', help='Start Date', required=True)
    #     parser.add_argument('--start_date', help='Start Date', required=False)
    #     parser.add_argument('--end_date', help='Start Date', required=False)
    # ----------------------------------------------------------------------


    # ----------------------------------------------------------------------
    # TODO: etl_client.close() before finally
    # Fun/danger: this is caused by fixing exceptions with a facile approach
    # ----------------------------------------------------------------------

    # ----------------------------------------------------------------------
    # Hidden blockers (depends on an unmigrated table)
    # ----------------------------------------------------------------------
    ["d_team_list_sales_gsheet"].each do |table|
      detect_and_print_lines lines, "HIDDEN TABLE DEPENDENCY: #{table}", /#{table}/i, essential: true
    end

    # ----------------------------------------------------------------------
    # main() bootstrapper
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "main() bootstrapper", /if\s*__name__\s*==\s*["']__main__["']\s*:/

    # ----------------------------------------------------------------------
    # WARN if job is mcloud or sf but does not have --skip_upload
    # ----------------------------------------------------------------------
    detect_missing_skip_upload

    # ----------------------------------------------------------------------
    # CREATE TABLE a (LIKE b) -> CREATE TABLE a LIKE b
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "CREATE TABLE (LIKE)", /\bCREATE\s+TABLE\b.*\(LIKE/i, suggest: :replace_inline_create_table_like, essential: true
    detect_and_print_multilines lines, "CREATE TABLE (LIKE)", [/\bCREATE\s+TABLE\b.*\(/i, /LIKE/i], essential: true
    detect_and_print_multilines lines, "CREATE TABLE (LIKE)", [/\bCREATE\s+TABLE\b/i, /\(\s*LIKE/i], essential: true

    # ----------------------------------------------------------------------
    # ENCODE
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "ENCODE", /\bencode\s+\S+/i, essential: true

    # ----------------------------------------------------------------------
    # GREATEST/LEAST -> NVL(...)
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "GREATEST/LEAST", /\b(greatest|least)\b/i, essential: true

    # ----------------------------------------------------------------------
    # TABS
    # ----------------------------------------------------------------------
    detect_and_print_lines lines, "TABS", /\t/
  end
end


if __FILE__ == $0
  SnowflakeManglerApp.new.run
end
