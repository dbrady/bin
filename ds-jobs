#!/usr/bin/env ruby
require 'colorize'
require 'csv'
require 'optimist'
require 'text-table'

hostname=`hostname`.strip

opts = Optimist::options do
  opt :warehouse, "Warehouse", type: :string, default: "snowflake"
  opt :limit, "Max jobs to return", default: 20
  opt :job, "Job name or path/to/job.py", type: :string
  opt :host, "Hostname"
  opt :mine, "Same as --host=#{hostname}", default: false
  opt :notes, "Show notes column", default: false
  opt :times, "Show elapsed times (generate huge-ass query)", default: false
end
Optimist::die "Cannot specify --host and --mine at same time" if opts[:mine] && opts[:host_given]
opts[:job] = opts[:job].gsub(%r|/|,".").sub(/\.py$/,'') if opts[:job_given]

puts opts.inspect
opts[:host] = hostname if opts[:mine]

# optional columns
notes = ", notes " if opts[:notes]

# where clause
wheres = []
wheres << "job='#{opts[:job]}'" if opts[:job_given]
wheres << "host='#{opts[:host]}'" if opts[:mine] || opts[:host_given]
where = if wheres.size.zero?
          ""
        else
          "WHERE " + wheres.join(" AND ")
        end

command = if opts[:times]
            %Q(dsquery --csv --warehouse snowflake --query "SELECT host, job, start_time, end_time, DATEDIFF('second', start_time, (CASE WHEN end_time IS NOT NULL THEN end_time ELSE CURRENT_TIMESTAMP END)) AS elapsed_seconds, success, job_failed_count, retry_count#{notes} FROM f_etl_job_details #{where} ORDER BY start_time DESC LIMIT #{opts[:limit]}").gsub(/\s+/, ' ')
          else
            %Q(dsquery --csv --warehouse snowflake --query "SELECT host, job, start_time, end_time, DATEDIFF('second', start_time, (CASE WHEN end_time IS NOT NULL THEN end_time ELSE CURRENT_TIMESTAMP END)) AS elapsed_seconds, success, job_failed_count, retry_count#{notes} FROM f_etl_job_details #{where} ORDER BY start_time DESC LIMIT #{opts[:limit]}").gsub(/\s+/, ' ')
          end

puts command.cyan
results = `#{command}`.strip

puts CSV.parse(results, headers: true).to_table(first_row_is_head: true)
